#!/usr/bin/env python3
"""
Evaluation runner for Prime Counter with Multiprocessing.

This evaluation script:
- Runs pytest tests on the tests/ folder for both before and after implementations
- Collects individual test results with pass/fail status
- Measures performance metrics (execution time, correctness)
- Generates structured reports with environment metadata

Run with:
    docker compose run --rm app python evaluation/evaluation.py [options]
"""
import os
import sys
import json
import uuid
import platform
import subprocess
from datetime import datetime
from pathlib import Path


def generate_run_id():
    """Generate a short unique run ID."""
    return uuid.uuid4().hex[:8]


def get_git_info():
    """Get git commit and branch information."""
    git_info = {"git_commit": "unknown", "git_branch": "unknown"}
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            git_info["git_commit"] = result.stdout.strip()[:8]
    except Exception:
        pass
    
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            git_info["git_branch"] = result.stdout.strip()
    except Exception:
        pass
    
    return git_info


def get_cpu_info():
    """Get detailed CPU information."""
    import multiprocessing as mp
    
    cpu_info = {
        "cores": mp.cpu_count(),
        "processor": platform.processor(),
    }
    
    # Try to get more detailed CPU info on Linux
    try:
        with open('/proc/cpuinfo', 'r') as f:
            for line in f:
                if 'model name' in line.lower():
                    cpu_info["model"] = line.split(':')[1].strip()
                    break
    except Exception:
        pass
    
    return cpu_info


def get_environment_info():
    """Collect environment information for the report."""
    git_info = get_git_info()
    cpu_info = get_cpu_info()
    
    return {
        "python_version": platform.python_version(),
        "platform": platform.platform(),
        "os": platform.system(),
        "os_release": platform.release(),
        "architecture": platform.machine(),
        "hostname": platform.node(),
        "cpu_cores": cpu_info["cores"],
        "cpu_processor": cpu_info["processor"],
        "cpu_model": cpu_info.get("model", "unknown"),
        "git_commit": git_info["git_commit"],
        "git_branch": git_info["git_branch"],
    }


def run_pytest_with_pythonpath(pythonpath, tests_dir, test_file, label):
    """
    Run pytest on specific test file with specific PYTHONPATH.
    
    Args:
        pythonpath: The PYTHONPATH to use for the tests
        tests_dir: Path to the tests directory
        test_file: Name of the test file to run (e.g., "test_after.py")
        label: Label for this test run (e.g., "before", "after")
    
    Returns:
        dict with test results
    """
    print(f"\n{'=' * 70}")
    print(f"RUNNING TESTS: {label.upper()}")
    print(f"{'=' * 70}")
    print(f"PYTHONPATH: {pythonpath}")
    print(f"Test file: {test_file}")
    
    test_path = tests_dir / test_file
    
    if not test_path.exists():
        print(f"‚ö†Ô∏è  Test file not found: {test_path}")
        return {
            "success": False,
            "exit_code": -1,
            "tests": [],
            "summary": {"error": f"Test file not found: {test_file}"},
            "stdout": "",
            "stderr": "",
        }
    
    # Decide whether to run via pytest or as a standalone Python script.
    # Some projects include a custom test runner (like `tests/test_after.py`) that
    # is intended to be executed directly rather than collected by pytest.
    # Detect that case and run the file with the Python interpreter to preserve
    # its intended behavior (multiprocessing entry points, custom prints, etc.).
    try:
        content = test_path.read_text()
    except Exception:
        content = ""

    run_as_script = False
    if "if __name__ == \"__main__\"" in content or "run_all_tests(" in content:
        run_as_script = True

    env = os.environ.copy()
    env["PYTHONPATH"] = pythonpath

    # Run as standalone script when detected
    if run_as_script:
        cmd = [sys.executable, str(test_path)]
    else:
        # Default: use pytest for standard test files
        cmd = [
            sys.executable, "-m", "pytest",
            str(test_path),
            "-v",
            "--tb=short",
            "-s",
        ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=str(Path(tests_dir).parent),
            env=env,
            timeout=300,
        )

        stdout = result.stdout
        stderr = result.stderr

        # Try to parse pytest-style results; if none found and we ran the
        # script directly, produce a minimal fallback summary but keep the
        # full raw output so callers can verify details.
        tests = parse_pytest_verbose_output(stdout)
        performance_metrics = extract_performance_metrics(stdout)

        if tests:
            passed = sum(1 for t in tests if t.get("outcome") == "passed")
            failed = sum(1 for t in tests if t.get("outcome") == "failed")
            errors = sum(1 for t in tests if t.get("outcome") == "error")
            skipped = sum(1 for t in tests if t.get("outcome") == "skipped")
            total = len(tests)
        else:
            # Heuristic for standalone runner: try to glean results from the
            # script's printed summary (e.g. "Results: 1 passed, 0 failed...")
            passed = failed = errors = skipped = 0
            total = 0

            # Look for a Results: X passed, Y failed, Z errors pattern
            import re
            m = re.search(r"Results:\s*(\d+)\s*passed,\s*(\d+)\s*failed,\s*(\d+)\s*errors,?\s*(\d*)\s*skipped?", stdout)
            if m:
                try:
                    passed = int(m.group(1))
                    failed = int(m.group(2))
                    errors = int(m.group(3))
                    skipped = int(m.group(4)) if m.group(4) else 0
                    total = passed + failed + errors + skipped
                except Exception:
                    passed = failed = errors = skipped = 0
                    total = 0

            # Fallback: detect ALL TESTS PASSED marker
            if total == 0 and ("ALL TESTS PASSED" in stdout or "‚úì ALL TESTS PASSED" in stdout):
                passed = 1
                failed = 0
                errors = 0
                skipped = 0
                total = 1

            # If still no data, but the process exited with 0, treat as success
            if total == 0 and result.returncode == 0:
                passed = 1
                failed = 0
                total = 1

            # Build a minimal synthetic test entry so callers have at least one
            # record to inspect. This helps downstream tooling expecting an
            # array of tests.
            tests = [{
                "nodeid": f"{test_file}::script_runner",
                "name": "script_runner",
                "outcome": "passed" if failed == 0 and result.returncode == 0 else ("failed" if failed > 0 else "error"),
            }]

        print(f"\nResults: {passed} passed, {failed} failed, {errors} errors, {skipped} skipped (total: {total})")

        # When pytest provided test details, print them; otherwise, print a
        # short summary from the script output.
        if tests and len(tests) > 0 and "script_runner" not in tests[0].get("nodeid", ""):
            for test in tests:
                status_icon = {
                    "passed": "‚úÖ",
                    "failed": "‚ùå",
                    "error": "üí•",
                    "skipped": "‚è≠Ô∏è"
                }.get(test.get("outcome"), "‚ùì")
                print(f"  {status_icon} {test.get('name', 'unknown')}: {test.get('outcome', 'unknown')}")
        else:
            # Print a short runner summary if present
            if "ALL TESTS PASSED" in stdout:
                print("  ‚úì ALL TESTS PASSED (script summary)")
            elif "SOME TESTS FAILED" in stdout or result.returncode != 0 or failed > 0:
                print("  ‚úó Some tests failed (script summary)")

        # Print performance summary if available
        if performance_metrics:
            print(f"\nPerformance Metrics:")
            for key, value in performance_metrics.items():
                print(f"  {key}: {value}")

        # Compose return payload. Keep truncated `stdout`/`stderr` for quick
        # display but include the full raw output in the `summary.raw_output`
        # field so reports contain complete evidence.
        return {
            "success": (passed > 0 and failed == 0) if total > 0 else (result.returncode == 0),
            "exit_code": result.returncode,
            "tests": tests,
            "summary": {
                "total": total,
                "passed": passed,
                "failed": failed,
                "errors": errors,
                "skipped": skipped,
                "raw_output": stdout,
            },
            "performance_metrics": performance_metrics,
            "stdout": stdout[-5000:] if len(stdout) > 5000 else stdout,
            "stderr": stderr[-2000:] if len(stderr) > 2000 else stderr,
        }

    except subprocess.TimeoutExpired:
        print("‚ùå Test execution timed out (> 5 minutes)")
        return {
            "success": False,
            "exit_code": -1,
            "tests": [],
            "summary": {"error": "Test execution timed out after 5 minutes"},
            "performance_metrics": {},
            "stdout": "",
            "stderr": "",
        }
    except Exception as e:
        print(f"‚ùå Error running tests: {e}")
        return {
            "success": False,
            "exit_code": -1,
            "tests": [],
            "summary": {"error": str(e)},
            "performance_metrics": {},
            "stdout": "",
            "stderr": "",
        }


def parse_pytest_verbose_output(output):
    """Parse pytest verbose output to extract test results."""
    tests = []
    lines = output.split('\n')
    
    for line in lines:
        line_stripped = line.strip()
        
        # Match lines like: tests/test_after.py::test_known_prime_counts PASSED
        if '::' in line_stripped and any(status in line_stripped for status in [' PASSED', ' FAILED', ' ERROR', ' SKIPPED']):
            outcome = None
            if ' PASSED' in line_stripped:
                outcome = "passed"
            elif ' FAILED' in line_stripped:
                outcome = "failed"
            elif ' ERROR' in line_stripped:
                outcome = "error"
            elif ' SKIPPED' in line_stripped:
                outcome = "skipped"
            
            if outcome:
                # Extract nodeid (everything before the status)
                for status_word in [' PASSED', ' FAILED', ' ERROR', ' SKIPPED']:
                    if status_word in line_stripped:
                        nodeid = line_stripped.split(status_word)[0].strip()
                        break
                
                tests.append({
                    "nodeid": nodeid,
                    "name": nodeid.split("::")[-1] if "::" in nodeid else nodeid,
                    "outcome": outcome,
                })
    
    return tests


def extract_performance_metrics(output):
    """Extract performance metrics from test output."""
    metrics = {}
    lines = output.split('\n')
    
    for line in lines:
        # Look for execution time
        if 'Time:' in line:
            try:
                parts = line.split('Time:')[1].strip()
                time_str = parts.split('s')[0].strip()
                metrics['execution_time_seconds'] = float(time_str)
                
                if 'on' in parts and 'cores' in parts:
                    cores_str = parts.split('on')[1].split('cores')[0].strip()
                    metrics['cpu_cores_used'] = int(cores_str)
            except Exception:
                print('passed')
                pass
        
        # Look for prime count
        if 'Primes found:' in line or 'primes' in line.lower():
            try:
                # Extract count like "Primes found: 664579"
                if ':' in line:
                    count_str = line.split(':')[-1].strip()
                    # Remove any non-digit characters
                    count_str = ''.join(c for c in count_str if c.isdigit())
                    if count_str:
                        metrics['primes_found'] = int(count_str)
            except Exception:
                pass
        
        # Look for performance tier
        if any(tier in line for tier in ['EXCELLENT', 'GOOD', 'ACCEPTABLE', 'EXCEPTIONAL']):
            if 'EXCEPTIONAL' in line:
                metrics['performance_tier'] = 'exceptional'
            elif 'EXCELLENT' in line:
                metrics['performance_tier'] = 'excellent'
            elif 'GOOD' in line:
                metrics['performance_tier'] = 'good'
            elif 'ACCEPTABLE' in line:
                metrics['performance_tier'] = 'acceptable'
    
    return metrics


def run_evaluation():
    """
    Run complete evaluation for both implementations.
    
    Returns dict with test results from both before and after implementations.
    """
    print(f"\n{'=' * 70}")
    print("PRIME COUNTER EVALUATION")
    print(f"{'=' * 70}")
    
    project_root = Path(__file__).parent.parent
    tests_dir = project_root / "tests"
    
    # Check if test files exist
    test_after_path = tests_dir / "test_after.py"
    
    has_after = test_after_path.exists()
    
    print(f"\nTest files found:")
    print(f"  test_after.py: {'‚úÖ' if has_after else '‚ùå'}")
    
    results = {}
    
    # Run AFTER tests
    if has_after:
        after_pythonpath = str(project_root / "repository_after")
        results["after"] = run_pytest_with_pythonpath(
            after_pythonpath,
            tests_dir,
            "test_after.py",
            "after (repository_after)"
        )
    else:
        print("\n‚ùå AFTER tests not found (test_after.py required)")
        results["after"] = {
            "success": False,
            "summary": {"error": "test_after.py not found"},
        }
    
    # Build comparison
    comparison = {}
    
    if results.get("after"):
        comparison.update({
            "after_tests_passed": results["after"].get("success", False),
            "after_total": results["after"].get("summary", {}).get("total", 0),
            "after_passed": results["after"].get("summary", {}).get("passed", 0),
            "after_failed": results["after"].get("summary", {}).get("failed", 0),
        })
        
        # Add performance comparison
        after_perf = results["after"].get("performance_metrics", {})
        if after_perf:
            comparison["performance"] = after_perf
    
    # Print summary
    print(f"\n{'=' * 70}")
    print("EVALUATION SUMMARY")
    print(f"{'=' * 70}")
    
    if results.get("before"):
        print(f"\nBefore Implementation (repository_before):")
        print(f"  Overall: {'‚úÖ PASSED' if results['before'].get('success') else '‚ùå FAILED'}")
        print(f"  Tests: {comparison.get('before_passed', 0)}/{comparison.get('before_total', 0)} passed")
    
    if results.get("after"):
        print(f"\nAfter Implementation (repository_after):")
        print(f"  Overall: {'‚úÖ PASSED' if results['after'].get('success') else '‚ùå FAILED'}")
        print(f"  Tests: {comparison.get('after_passed', 0)}/{comparison.get('after_total', 0)} passed")
        
        if comparison.get("performance"):
            perf = comparison["performance"]
            print(f"\n  Performance:")
            if "execution_time_seconds" in perf:
                print(f"    Time: {perf['execution_time_seconds']:.3f}s")
            if "performance_tier" in perf:
                print(f"    Tier: {perf['performance_tier'].upper()}")
            if "primes_found" in perf:
                print(f"    Primes: {perf['primes_found']:,}")
    
    # Determine expected behavior
    print(f"\n{'=' * 70}")
    print("EXPECTED BEHAVIOR CHECK")
    print(f"{'=' * 70}")
    
    if results.get("after"):
        if results["after"].get("success"):
            print("‚úÖ After implementation: All tests passed (expected)")
        else:
            print("‚ùå After implementation: Some tests failed")
            after_summary = results["after"].get("summary", {})
            if after_summary.get("failed", 0) > 0:
                print(f"   {after_summary['failed']} test(s) failed")
            if "error" in after_summary:
                print(f"   Error: {after_summary['error']}")
    
    results["comparison"] = comparison
    return results


def generate_output_path():
    """Generate output path in format: evaluation/YYYY-MM-DD/HH-MM-SS/report.json"""
    now = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H-%M-%S")
    
    project_root = Path(__file__).parent.parent
    output_dir = project_root / "evaluation" / date_str / time_str
    output_dir.mkdir(parents=True, exist_ok=True)
    
    return output_dir / "report.json"


def main():
    """Main entry point for evaluation."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Run prime counter evaluation")
    parser.add_argument(
        "--output", 
        type=str, 
        default=None, 
        help="Output JSON file path (default: evaluation/YYYY-MM-DD/HH-MM-SS/report.json)"
    )
    
    args = parser.parse_args()
    
    # Generate run ID and timestamps
    run_id = generate_run_id()
    started_at = datetime.now()
    
    print(f"Run ID: {run_id}")
    print(f"Started at: {started_at.isoformat()}")
    
    try:
        results = run_evaluation()
        
        # Success if after implementation passes all tests
        success = results.get("after", {}).get("success", False)
        error_message = None if success else "After implementation tests failed"
        
    except Exception as e:
        import traceback
        print(f"\nERROR: {str(e)}")
        traceback.print_exc()
        results = None
        success = False
        error_message = str(e)
    
    finished_at = datetime.now()
    duration = (finished_at - started_at).total_seconds()
    
    # Collect environment information
    environment = get_environment_info()
    
    # Build report
    report = {
        "run_id": run_id,
        "started_at": started_at.isoformat(),
        "finished_at": finished_at.isoformat(),
        "duration_seconds": round(duration, 6),
        "success": success,
        "error": error_message,
        "environment": environment,
        "results": results,
    }
    
    # Determine output path
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = generate_output_path()
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w") as f:
        json.dump(report, f, indent=2)
    print(f"\n‚úÖ Report saved to: {output_path}")
    
    print(f"\n{'=' * 70}")
    print(f"EVALUATION COMPLETE")
    print(f"{'=' * 70}")
    print(f"Run ID: {run_id}")
    print(f"Duration: {duration:.2f}s")
    print(f"Success: {'‚úÖ YES' if success else '‚ùå NO'}")
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
