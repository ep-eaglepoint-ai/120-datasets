diff --git a/repository_before/__init__.py b/repository_after/__init__.py
index e69de29b..9cf2c9d7 100644
--- a/repository_before/__init__.py
+++ b/repository_after/__init__.py
@@ -0,0 +1,3 @@
+from .main import aggregate_hourly_unique_visitors
+
+__all__ = ["aggregate_hourly_unique_visitors"]
diff --git a/repository_before/main.py b/repository_after/main.py
index f419d203..29376bfe 100644
--- a/repository_before/main.py
+++ b/repository_after/main.py
@@ -1,19 +1,63 @@
+"""
+Hourly unique visitor aggregation (highly optimized).
+
+PERFORMANCE BOTTLENECKS AND REMEDIES:
+------------------------------------
+1. Primary Bottleneck: String Formatting (strftime) per event.
+   Calling strftime() or replace() millions of times is slow due to Python's
+   object creation and formatting logic. This refactor extracts date parts
+   directly as integers and caches the formatted string lazily.
+2. Secondary Bottleneck: Multiple small containers (dict-of-dict-of-set).
+   Allocating thousands of sets causes GC churn. While a single flat set of
+   triples (hour_key, page, visitor) is easier to manage, it can bloat memory
+   due to redundant string storage. This version uses a single flat "seen" set
+   but keeps keys as compact as possible.
+3. Logical Bottleneck: Two-pass processing.
+   Building full sets then traversing again for counts is redundant.
+   This version increments counts on first-seen in a single pass.
+"""
+
+from collections import defaultdict
+
+
 def aggregate_hourly_unique_visitors(events):
-    # events is list of dicts: {'timestamp': datetime, 'page_url': str, 'visitor_id': str, ...}
-    result = {}
+    """
+    Optimized aggregation of hourly unique visitors per page.
+
+    Complexity: O(n) single pass with low constants.
+    Memory: O(u) where u is number of unique (hour, page, visitor) triples.
+    """
+    # Result structure: hour_str -> page_url -> count
+    result = defaultdict(lambda: defaultdict(int))
+    
+    # 1. Nesting 'seen' sets is more memory-efficient than a flat set of tuples
+    # as it avoids creating million of triple-tuple objects and deduplicates
+    # hour/page pointers.
+    seen = defaultdict(lambda: defaultdict(set))
+    
+    # 2. Cache for formatted hour strings to avoid redundant strftime calls.
+    hour_str_cache = {}
+
     for event in events:
-        hour_key = event['timestamp'].strftime('%Y-%m-%d %H:00')
-        page = event['page_url']
-        visitor = event['visitor_id']
-        if hour_key not in result:
-            result[hour_key] = {}
-        if page not in result[hour_key]:
-            result[hour_key][page] = set()
-        result[hour_key][page].add(visitor)
-    # Convert sets to counts
-    final = {}
-    for hour, pages in result.items():
-        final[hour] = {}
-        for page, visitors in pages.items():
-            final[hour][page] = len(visitors)
-    return final
\ No newline at end of file
+        ts = event["timestamp"]
+        page = event["page_url"]
+        visitor = event["visitor_id"]
+
+        # Fast integer extraction
+        h_tuple = (ts.year, ts.month, ts.day, ts.hour)
+        
+        # Guard against double counting in a single pass
+        hour_seen = seen[h_tuple]
+        page_seen = hour_seen[page]
+        
+        if visitor not in page_seen:
+            page_seen.add(visitor)
+            
+            # Resolve formatted hour string (lazy-cache)
+            if h_tuple not in hour_str_cache:
+                hour_str_cache[h_tuple] = ts.strftime("%Y-%m-%d %H:00")
+            
+            result[hour_str_cache[h_tuple]][page] += 1
+
+    # Return plain dict[str, dict[str, int]]
+    return {h: dict(p) for h, p in result.items()}
