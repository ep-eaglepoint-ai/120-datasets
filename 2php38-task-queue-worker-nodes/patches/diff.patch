diff -ruN repository_before/__init__.py repository_after/__init__.py
--- repository_before/__init__.py	2026-02-14 21:33:24
+++ repository_after/__init__.py	2026-02-14 21:33:24
@@ -0,0 +1,170 @@
+"""Distributed Task Queue System.
+
+A high-performance distributed task queue with intelligent priority scheduling,
+automatic retry mechanisms, and comprehensive job lifecycle management.
+
+Uses Redis Streams for reliable message delivery, structlog for structured logging,
+and Prometheus client for metrics exposition.
+"""
+from .client import AsyncTaskQueue, TaskQueue
+from .dependencies import CircularDependencyError, DependencyGraph, DependencyResolver
+from .models import (
+    Job,
+    JobPayload,
+    JobResult,
+    JobStatus,
+    Priority,
+    QueueStats,
+    RetryConfig,
+    RetryStrategy,
+    TypedJob,
+    WorkerInfo,
+)
+from .retry import (
+    ExponentialBackoffStrategy,
+    FixedDelayStrategy,
+    RetryDecision,
+    RetryManager,
+    RetryScheduler,
+    RetryStrategyFactory,
+    RetryStrategyHandler,
+)
+from .scheduler import (
+    BulkJobSubmitter,
+    CronExpression,
+    DelayedJobScheduler,
+    RecurringJobScheduler,
+    UniquenessConstraint,
+)
+from .serialization import (
+    CompressedSerializer,
+    JSONSerializer,
+    MessagePackSerializer,
+    PayloadEncoder,
+    PickleSerializer,
+    SerializationFormat,
+    Serializer,
+    SerializerFactory,
+)
+from .worker import (
+    DistributedLock,
+    GracefulShutdown,
+    LeaderElection,
+    WorkerNode,
+    WorkerProcess,
+    WorkerRegistry,
+    WorkStealing,
+)
+
+from .redis_backend import (
+    RedisConfig,
+    RedisConnection,
+    RedisStreamsQueue,
+    RedisDistributedLock,
+    RedisLeaderElection,
+)
+from .logging_config import (
+    configure_logging,
+    get_logger,
+    bind_context,
+    clear_context,
+)
+from .prometheus_metrics import (
+    TaskQueuePrometheusMetrics,
+    get_metrics,
+)
+from .multiprocess_worker import (
+    MultiprocessWorkerPool,
+    AsyncWorkerPool,
+    HybridWorkerPool,
+)
+from .alerting import (
+    Alert,
+    AlertSeverity,
+    AlertHandler,
+    AlertManager,
+    LogAlertHandler,
+    WebhookAlertHandler,
+    CallbackAlertHandler,
+    get_alert_manager,
+)
+
+__version__ = "1.0.0"
+__all__ = [
+    # Client
+    "TaskQueue",
+    "AsyncTaskQueue",
+    # Models
+    "Job",
+    "JobPayload",
+    "JobResult",
+    "JobStatus",
+    "Priority",
+    "QueueStats",
+    "RetryConfig",
+    "RetryStrategy",
+    "WorkerInfo",
+    "TypedJob",
+    # Dependencies
+    "DependencyGraph",
+    "DependencyResolver",
+    "CircularDependencyError",
+    # Retry
+    "RetryManager",
+    "RetryScheduler",
+    "RetryDecision",
+    "RetryStrategyHandler",
+    "RetryStrategyFactory",
+    "FixedDelayStrategy",
+    "ExponentialBackoffStrategy",
+    # Scheduler
+    "DelayedJobScheduler",
+    "RecurringJobScheduler",
+    "BulkJobSubmitter",
+    "CronExpression",
+    "UniquenessConstraint",
+    # Worker
+    "WorkerProcess",
+    "WorkerNode",
+    "WorkerRegistry",
+    "WorkStealing",
+    "GracefulShutdown",
+    "LeaderElection",
+    "DistributedLock",
+    # Serialization
+    "Serializer",
+    "SerializerFactory",
+    "SerializationFormat",
+    "JSONSerializer",
+    "MessagePackSerializer",
+    "PickleSerializer",
+    "CompressedSerializer",
+    "PayloadEncoder",
+    # Redis Backend (distributed queue)
+    "RedisConfig",
+    "RedisConnection",
+    "RedisStreamsQueue",
+    "RedisDistributedLock",
+    "RedisLeaderElection",
+    # Logging (structlog)
+    "configure_logging",
+    "get_logger",
+    "bind_context",
+    "clear_context",
+    # Prometheus Metrics (official client)
+    "TaskQueuePrometheusMetrics",
+    "get_metrics",
+    # Multiprocessing Workers
+    "MultiprocessWorkerPool",
+    "AsyncWorkerPool",
+    "HybridWorkerPool",
+    # Alerting
+    "Alert",
+    "AlertSeverity",
+    "AlertHandler",
+    "AlertManager",
+    "LogAlertHandler",
+    "WebhookAlertHandler",
+    "CallbackAlertHandler",
+    "get_alert_manager",
+]
Binary files repository_before/__pycache__/__init__.cpython-311.pyc and repository_after/__pycache__/__init__.cpython-311.pyc differ
Binary files repository_before/__pycache__/alerting.cpython-311.pyc and repository_after/__pycache__/alerting.cpython-311.pyc differ
Binary files repository_before/__pycache__/api.cpython-311.pyc and repository_after/__pycache__/api.cpython-311.pyc differ
Binary files repository_before/__pycache__/client.cpython-311.pyc and repository_after/__pycache__/client.cpython-311.pyc differ
Binary files repository_before/__pycache__/dependencies.cpython-311.pyc and repository_after/__pycache__/dependencies.cpython-311.pyc differ
Binary files repository_before/__pycache__/logging_config.cpython-311.pyc and repository_after/__pycache__/logging_config.cpython-311.pyc differ
Binary files repository_before/__pycache__/models.cpython-311.pyc and repository_after/__pycache__/models.cpython-311.pyc differ
Binary files repository_before/__pycache__/multiprocess_worker.cpython-311.pyc and repository_after/__pycache__/multiprocess_worker.cpython-311.pyc differ
Binary files repository_before/__pycache__/prometheus_metrics.cpython-311.pyc and repository_after/__pycache__/prometheus_metrics.cpython-311.pyc differ
Binary files repository_before/__pycache__/redis_backend.cpython-311.pyc and repository_after/__pycache__/redis_backend.cpython-311.pyc differ
Binary files repository_before/__pycache__/retry.cpython-311.pyc and repository_after/__pycache__/retry.cpython-311.pyc differ
Binary files repository_before/__pycache__/scheduler.cpython-311.pyc and repository_after/__pycache__/scheduler.cpython-311.pyc differ
Binary files repository_before/__pycache__/serialization.cpython-311.pyc and repository_after/__pycache__/serialization.cpython-311.pyc differ
Binary files repository_before/__pycache__/worker.cpython-311.pyc and repository_after/__pycache__/worker.cpython-311.pyc differ
diff -ruN repository_before/alerting.py repository_after/alerting.py
--- repository_before/alerting.py	1970-01-01 03:00:00
+++ repository_after/alerting.py	2026-02-14 21:33:24
@@ -0,0 +1,277 @@
+"""Alerting and failure notification callbacks."""
+from __future__ import annotations
+
+import asyncio
+from abc import ABC, abstractmethod
+from dataclasses import dataclass, field
+from datetime import datetime
+from enum import Enum
+from typing import Any, Callable, Dict, List, Optional, Union
+
+from .logging_config import get_logger
+from .models import Job, Priority
+
+logger = get_logger(__name__)
+
+
+class AlertSeverity(str, Enum):
+    """Alert severity levels."""
+    INFO = "info"
+    WARNING = "warning"
+    ERROR = "error"
+    CRITICAL = "critical"
+
+
+@dataclass
+class Alert:
+    """Alert data structure."""
+    id: str
+    severity: AlertSeverity
+    title: str
+    message: str
+    job_id: Optional[str] = None
+    job_name: Optional[str] = None
+    worker_id: Optional[str] = None
+    error: Optional[str] = None
+    timestamp: datetime = field(default_factory=datetime.utcnow)
+    metadata: Dict[str, Any] = field(default_factory=dict)
+
+
+class AlertHandler(ABC):
+    """Abstract base class for alert handlers."""
+    
+    @abstractmethod
+    async def send(self, alert: Alert) -> bool:
+        """Send an alert. Returns True if successful."""
+        pass
+
+
+class LogAlertHandler(AlertHandler):
+    """Alert handler that logs to structlog."""
+    
+    async def send(self, alert: Alert) -> bool:
+        """Log the alert."""
+        log_method = {
+            AlertSeverity.INFO: logger.info,
+            AlertSeverity.WARNING: logger.warning,
+            AlertSeverity.ERROR: logger.error,
+            AlertSeverity.CRITICAL: logger.critical,
+        }.get(alert.severity, logger.error)
+        
+        log_method(
+            "alert",
+            alert_id=alert.id,
+            severity=alert.severity.value,
+            title=alert.title,
+            message=alert.message,
+            job_id=alert.job_id,
+            worker_id=alert.worker_id,
+            error=alert.error,
+        )
+        return True
+
+
+class WebhookAlertHandler(AlertHandler):
+    """Alert handler that sends to a webhook URL."""
+    
+    def __init__(self, webhook_url: str, headers: Optional[Dict[str, str]] = None):
+        self._webhook_url = webhook_url
+        self._headers = headers or {"Content-Type": "application/json"}
+    
+    async def send(self, alert: Alert) -> bool:
+        """Send alert to webhook."""
+        try:
+            import httpx
+            
+            payload = {
+                "id": alert.id,
+                "severity": alert.severity.value,
+                "title": alert.title,
+                "message": alert.message,
+                "job_id": alert.job_id,
+                "job_name": alert.job_name,
+                "worker_id": alert.worker_id,
+                "error": alert.error,
+                "timestamp": alert.timestamp.isoformat(),
+                "metadata": alert.metadata,
+            }
+            
+            async with httpx.AsyncClient() as client:
+                response = await client.post(
+                    self._webhook_url,
+                    json=payload,
+                    headers=self._headers,
+                    timeout=10.0,
+                )
+                
+                if response.status_code < 300:
+                    logger.debug("webhook_alert_sent", alert_id=alert.id)
+                    return True
+                else:
+                    logger.warning(
+                        "webhook_alert_failed",
+                        alert_id=alert.id,
+                        status_code=response.status_code,
+                    )
+                    return False
+                    
+        except Exception as e:
+            logger.error("webhook_alert_error", alert_id=alert.id, error=str(e))
+            return False
+
+
+class CallbackAlertHandler(AlertHandler):
+    """Alert handler that calls a user-provided callback."""
+    
+    def __init__(self, callback: Callable[[Alert], Union[bool, Any]]):
+        self._callback = callback
+    
+    async def send(self, alert: Alert) -> bool:
+        """Call the callback with the alert."""
+        try:
+            if asyncio.iscoroutinefunction(self._callback):
+                result = await self._callback(alert)
+            else:
+                result = self._callback(alert)
+            return bool(result) if result is not None else True
+        except Exception as e:
+            logger.error("callback_alert_error", alert_id=alert.id, error=str(e))
+            return False
+
+
+class AlertManager:
+    """Manages alert handlers and dispatches alerts."""
+    
+    def __init__(self):
+        self._handlers: List[AlertHandler] = []
+        self._alert_count = 0
+        self._default_handler = LogAlertHandler()
+    
+    def add_handler(self, handler: AlertHandler):
+        """Add an alert handler."""
+        self._handlers.append(handler)
+        logger.info("alert_handler_added", handler_type=type(handler).__name__)
+    
+    def remove_handler(self, handler: AlertHandler):
+        """Remove an alert handler."""
+        if handler in self._handlers:
+            self._handlers.remove(handler)
+    
+    async def send_alert(
+        self,
+        severity: AlertSeverity,
+        title: str,
+        message: str,
+        job: Optional[Job] = None,
+        worker_id: Optional[str] = None,
+        error: Optional[str] = None,
+        metadata: Optional[Dict[str, Any]] = None,
+    ) -> str:
+        """Create and send an alert to all handlers."""
+        import uuid
+        
+        self._alert_count += 1
+        alert_id = f"alert-{uuid.uuid4().hex[:8]}"
+        
+        alert = Alert(
+            id=alert_id,
+            severity=severity,
+            title=title,
+            message=message,
+            job_id=job.id if job else None,
+            job_name=job.name if job else None,
+            worker_id=worker_id,
+            error=error,
+            metadata=metadata or {},
+        )
+        
+        handlers = self._handlers if self._handlers else [self._default_handler]
+        
+        tasks = [handler.send(alert) for handler in handlers]
+        await asyncio.gather(*tasks, return_exceptions=True)
+        
+        return alert_id
+    
+    async def alert_job_failed(
+        self,
+        job: Job,
+        error: str,
+        will_retry: bool = False,
+    ):
+        """Send alert for job failure."""
+        severity = AlertSeverity.WARNING if will_retry else AlertSeverity.ERROR
+        
+        await self.send_alert(
+            severity=severity,
+            title=f"Job Failed: {job.name}",
+            message=f"Job {job.id} failed after attempt {job.attempt}",
+            job=job,
+            error=error,
+            metadata={"will_retry": will_retry, "attempt": job.attempt},
+        )
+    
+    async def alert_job_dead_lettered(self, job: Job, error: str):
+        """Send alert for job sent to DLQ."""
+        await self.send_alert(
+            severity=AlertSeverity.ERROR,
+            title=f"Job Dead-Lettered: {job.name}",
+            message=f"Job {job.id} exhausted all retries and was sent to DLQ",
+            job=job,
+            error=error,
+            metadata={"final_attempt": job.attempt},
+        )
+    
+    async def alert_worker_unhealthy(self, worker_id: str, reason: str):
+        """Send alert for unhealthy worker."""
+        await self.send_alert(
+            severity=AlertSeverity.WARNING,
+            title=f"Worker Unhealthy: {worker_id}",
+            message=reason,
+            worker_id=worker_id,
+        )
+    
+    async def alert_worker_dead(self, worker_id: str, jobs_affected: int):
+        """Send alert for dead worker."""
+        await self.send_alert(
+            severity=AlertSeverity.CRITICAL,
+            title=f"Worker Dead: {worker_id}",
+            message=f"Worker {worker_id} is unresponsive. {jobs_affected} jobs may need reassignment.",
+            worker_id=worker_id,
+            metadata={"jobs_affected": jobs_affected},
+        )
+    
+    async def alert_queue_depth_high(self, priority: Priority, depth: int, threshold: int):
+        """Send alert for high queue depth."""
+        await self.send_alert(
+            severity=AlertSeverity.WARNING,
+            title=f"High Queue Depth: {priority.name}",
+            message=f"Queue depth for {priority.name} is {depth}, exceeding threshold of {threshold}",
+            metadata={"priority": priority.name, "depth": depth, "threshold": threshold},
+        )
+    
+    async def alert_throughput_degraded(
+        self, 
+        current_throughput: float, 
+        expected_throughput: float,
+    ):
+        """Send alert for degraded throughput."""
+        await self.send_alert(
+            severity=AlertSeverity.WARNING,
+            title="Throughput Degraded",
+            message=f"Current throughput {current_throughput:.2f}/s is below expected {expected_throughput:.2f}/s",
+            metadata={
+                "current_throughput": current_throughput,
+                "expected_throughput": expected_throughput,
+            },
+        )
+
+
+_alert_manager: Optional[AlertManager] = None
+
+
+def get_alert_manager() -> AlertManager:
+    """Get global alert manager instance."""
+    global _alert_manager
+    if _alert_manager is None:
+        _alert_manager = AlertManager()
+    return _alert_manager
diff -ruN repository_before/api.py repository_after/api.py
--- repository_before/api.py	1970-01-01 03:00:00
+++ repository_after/api.py	2026-02-14 21:33:24
@@ -0,0 +1,383 @@
+"""FastAPI REST API for job inspection and queue management."""
+from __future__ import annotations
+
+from datetime import datetime
+from typing import Any, Dict, List, Optional
+
+from fastapi import FastAPI, HTTPException, Query, BackgroundTasks
+from fastapi.responses import Response
+from pydantic import BaseModel, Field
+
+from .logging_config import get_logger
+from .models import Job, JobStatus, Priority, RetryConfig, QueueStats
+from .prometheus_metrics import get_metrics
+
+logger = get_logger(__name__)
+
+app = FastAPI(
+    title="Task Queue API",
+    description="REST API for distributed task queue management",
+    version="1.0.0",
+)
+
+
+class JobSubmitRequest(BaseModel):
+    """Request model for job submission."""
+    name: str = Field(..., description="Job name/type")
+    payload: Dict[str, Any] = Field(default_factory=dict, description="Job payload")
+    priority: str = Field(default="NORMAL", description="Job priority level")
+    delay_ms: int = Field(default=0, ge=0, description="Delay in milliseconds")
+    depends_on: List[str] = Field(default_factory=list, description="Job dependencies")
+    unique_key: Optional[str] = Field(default=None, description="Unique constraint key")
+    retry_max_attempts: int = Field(default=3, ge=1, description="Max retry attempts")
+    retry_base_delay_ms: int = Field(default=1000, ge=0, description="Base retry delay")
+
+
+class JobSubmitResponse(BaseModel):
+    """Response model for job submission."""
+    job_id: str
+    status: str
+    created_at: str
+
+
+class JobResponse(BaseModel):
+    """Response model for job details."""
+    id: str
+    name: str
+    status: str
+    priority: str
+    payload: Dict[str, Any]
+    attempt: int
+    created_at: str
+    started_at: Optional[str] = None
+    completed_at: Optional[str] = None
+    last_error: Optional[str] = None
+    depends_on: List[str] = []
+
+
+class JobListResponse(BaseModel):
+    """Response model for job list."""
+    jobs: List[JobResponse]
+    total: int
+    cursor: Optional[str] = None
+
+
+class QueueStatsResponse(BaseModel):
+    """Response model for queue statistics."""
+    total_jobs: int
+    pending_jobs: int
+    running_jobs: int
+    completed_jobs: int
+    failed_jobs: int
+    dead_jobs: int
+    avg_processing_time_ms: float
+    throughput_per_second: float
+    queue_depths: Dict[str, int]
+    worker_count: int
+
+
+class WorkerResponse(BaseModel):
+    """Response model for worker details."""
+    id: str
+    name: str
+    host: str
+    port: int
+    status: str
+    current_jobs: int
+    max_concurrent_jobs: int
+    last_heartbeat: str
+
+
+class WorkerListResponse(BaseModel):
+    """Response model for worker list."""
+    workers: List[WorkerResponse]
+    total: int
+
+
+_task_queue = None
+
+
+def set_task_queue(queue):
+    """Set the task queue instance for the API."""
+    global _task_queue
+    _task_queue = queue
+
+
+def get_task_queue():
+    """Get the task queue instance."""
+    if _task_queue is None:
+        raise HTTPException(status_code=503, detail="Task queue not initialized")
+    return _task_queue
+
+
+@app.post("/jobs", response_model=JobSubmitResponse, status_code=201)
+async def submit_job(request: JobSubmitRequest):
+    """Submit a new job to the queue."""
+    queue = get_task_queue()
+    
+    try:
+        priority = Priority[request.priority.upper()]
+    except KeyError:
+        raise HTTPException(
+            status_code=400, 
+            detail=f"Invalid priority: {request.priority}. Valid values: {[p.name for p in Priority]}"
+        )
+    
+    try:
+        job_id = queue.submit(
+            name=request.name,
+            payload=request.payload,
+            priority=priority,
+            delay_ms=request.delay_ms,
+            depends_on=request.depends_on,
+            unique_key=request.unique_key,
+            retry_config=RetryConfig(
+                max_attempts=request.retry_max_attempts,
+                base_delay_ms=request.retry_base_delay_ms,
+            ),
+        )
+        
+        logger.info("job_submitted_via_api", job_id=job_id, name=request.name)
+        
+        return JobSubmitResponse(
+            job_id=job_id,
+            status="PENDING",
+            created_at=datetime.utcnow().isoformat(),
+        )
+    except ValueError as e:
+        raise HTTPException(status_code=400, detail=str(e))
+    except Exception as e:
+        logger.error("job_submit_error", error=str(e))
+        raise HTTPException(status_code=500, detail="Failed to submit job")
+
+
+@app.get("/jobs/{job_id}", response_model=JobResponse)
+async def get_job(job_id: str):
+    """Get job details by ID."""
+    queue = get_task_queue()
+    
+    job = queue.get_job(job_id)
+    if not job:
+        raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
+    
+    priority = Priority(job.priority) if isinstance(job.priority, int) else job.priority
+    
+    return JobResponse(
+        id=job.id,
+        name=job.name,
+        status=job.status.value if isinstance(job.status, JobStatus) else job.status,
+        priority=priority.name,
+        payload=job.payload,
+        attempt=job.attempt,
+        created_at=job.created_at.isoformat() if job.created_at else "",
+        started_at=job.started_at.isoformat() if job.started_at else None,
+        completed_at=job.completed_at.isoformat() if job.completed_at else None,
+        last_error=job.last_error,
+        depends_on=job.depends_on or [],
+    )
+
+
+@app.delete("/jobs/{job_id}", status_code=204)
+async def cancel_job(job_id: str):
+    """Cancel a pending job."""
+    queue = get_task_queue()
+    
+    if not queue.cancel_job(job_id):
+        raise HTTPException(status_code=404, detail=f"Job not found or cannot be cancelled: {job_id}")
+    
+    logger.info("job_cancelled_via_api", job_id=job_id)
+
+
+@app.put("/jobs/{job_id}/priority")
+async def update_job_priority(job_id: str, priority: str):
+    """Update job priority."""
+    queue = get_task_queue()
+    
+    try:
+        new_priority = Priority[priority.upper()]
+    except KeyError:
+        raise HTTPException(status_code=400, detail=f"Invalid priority: {priority}")
+    
+    if not queue.update_priority(job_id, new_priority):
+        raise HTTPException(status_code=404, detail=f"Job not found: {job_id}")
+    
+    return {"job_id": job_id, "priority": new_priority.name}
+
+
+@app.get("/jobs", response_model=JobListResponse)
+async def list_jobs(
+    status: Optional[str] = Query(None, description="Filter by status"),
+    priority: Optional[str] = Query(None, description="Filter by priority"),
+    limit: int = Query(50, ge=1, le=100, description="Max results"),
+    cursor: Optional[str] = Query(None, description="Pagination cursor"),
+):
+    """List jobs with optional filters."""
+    queue = get_task_queue()
+    
+    jobs = queue.list_jobs(
+        status=status,
+        priority=priority,
+        limit=limit,
+        cursor=cursor,
+    )
+    
+    job_responses = []
+    for job in jobs:
+        p = Priority(job.priority) if isinstance(job.priority, int) else job.priority
+        job_responses.append(JobResponse(
+            id=job.id,
+            name=job.name,
+            status=job.status.value if isinstance(job.status, JobStatus) else job.status,
+            priority=p.name,
+            payload=job.payload,
+            attempt=job.attempt,
+            created_at=job.created_at.isoformat() if job.created_at else "",
+            started_at=job.started_at.isoformat() if job.started_at else None,
+            completed_at=job.completed_at.isoformat() if job.completed_at else None,
+            last_error=job.last_error,
+            depends_on=job.depends_on or [],
+        ))
+    
+    next_cursor = jobs[-1].id if len(jobs) == limit else None
+    
+    return JobListResponse(
+        jobs=job_responses,
+        total=len(job_responses),
+        cursor=next_cursor,
+    )
+
+
+@app.post("/jobs/{job_id}/retry", response_model=JobResponse)
+async def retry_job(job_id: str):
+    """Retry a failed job."""
+    queue = get_task_queue()
+    
+    job = queue.retry_job(job_id)
+    if not job:
+        raise HTTPException(status_code=404, detail=f"Job not found or cannot be retried: {job_id}")
+    
+    logger.info("job_retried_via_api", job_id=job_id)
+    
+    p = Priority(job.priority) if isinstance(job.priority, int) else job.priority
+    return JobResponse(
+        id=job.id,
+        name=job.name,
+        status=job.status.value if isinstance(job.status, JobStatus) else job.status,
+        priority=p.name,
+        payload=job.payload,
+        attempt=job.attempt,
+        created_at=job.created_at.isoformat() if job.created_at else "",
+        depends_on=job.depends_on or [],
+    )
+
+
+@app.get("/stats", response_model=QueueStatsResponse)
+async def get_stats():
+    """Get queue statistics."""
+    queue = get_task_queue()
+    metrics = get_metrics()
+    
+    stats = queue.get_stats()
+    depths = queue.get_queue_depths()
+    
+    return QueueStatsResponse(
+        total_jobs=stats.total_jobs,
+        pending_jobs=stats.pending_jobs,
+        running_jobs=stats.running_jobs,
+        completed_jobs=stats.completed_jobs,
+        failed_jobs=stats.failed_jobs,
+        dead_jobs=stats.dead_jobs,
+        avg_processing_time_ms=stats.avg_processing_time_ms,
+        throughput_per_second=getattr(stats, 'throughput_per_second', 0.0),
+        queue_depths={p.name: d for p, d in depths.items()},
+        worker_count=queue.get_worker_count(),
+    )
+
+
+@app.get("/workers", response_model=WorkerListResponse)
+async def list_workers():
+    """List all registered workers."""
+    queue = get_task_queue()
+    
+    workers = queue.get_workers()
+    
+    worker_responses = []
+    for w in workers:
+        worker_responses.append(WorkerResponse(
+            id=w.info.id,
+            name=w.info.name,
+            host=w.info.host,
+            port=w.info.port,
+            status="active" if w.is_active() else "inactive",
+            current_jobs=len(w.info.current_jobs),
+            max_concurrent_jobs=w.info.max_concurrent_jobs,
+            last_heartbeat=w.info.last_heartbeat.isoformat() if w.info.last_heartbeat else "",
+        ))
+    
+    return WorkerListResponse(
+        workers=worker_responses,
+        total=len(worker_responses),
+    )
+
+
+@app.get("/dlq")
+async def list_dead_letter_queue(
+    limit: int = Query(50, ge=1, le=100),
+):
+    """List jobs in dead letter queue."""
+    queue = get_task_queue()
+    
+    dlq_jobs = queue.get_dlq_jobs(limit=limit)
+    
+    return {
+        "jobs": [
+            {
+                "id": job.id,
+                "name": job.name,
+                "last_error": job.last_error,
+                "attempts": job.attempt,
+            }
+            for job in dlq_jobs
+        ],
+        "total": len(dlq_jobs),
+    }
+
+
+@app.post("/dlq/{job_id}/requeue")
+async def requeue_from_dlq(job_id: str, reset_attempts: bool = Query(True)):
+    """Requeue a job from dead letter queue."""
+    queue = get_task_queue()
+    
+    job = queue.requeue_from_dlq(job_id, reset_attempts=reset_attempts)
+    if not job:
+        raise HTTPException(status_code=404, detail=f"Job not found in DLQ: {job_id}")
+    
+    return {"job_id": job.id, "status": "requeued"}
+
+
+@app.get("/metrics")
+async def prometheus_metrics():
+    """Expose Prometheus metrics."""
+    metrics = get_metrics()
+    return Response(
+        content=metrics.export(),
+        media_type="text/plain; charset=utf-8",
+    )
+
+
+@app.get("/health")
+async def health_check():
+    """Health check endpoint."""
+    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}
+
+
+@app.on_event("startup")
+async def startup_event():
+    """Initialize on startup."""
+    logger.info("api_startup")
+
+
+@app.on_event("shutdown")
+async def shutdown_event():
+    """Cleanup on shutdown."""
+    logger.info("api_shutdown")
diff -ruN repository_before/cli.py repository_after/cli.py
--- repository_before/cli.py	1970-01-01 03:00:00
+++ repository_after/cli.py	2026-02-14 21:33:24
@@ -0,0 +1,261 @@
+"""CLI interface for worker management."""
+from __future__ import annotations
+
+import argparse
+import asyncio
+import signal
+import sys
+import time
+from typing import Optional
+
+from .client import TaskQueue
+from .models import Job, JobResult, JobStatus, Priority
+from .worker import GracefulShutdown, WorkerNode, WorkerProcess
+
+
+def create_parser() -> argparse.ArgumentParser:
+    """Create the CLI argument parser."""
+    parser = argparse.ArgumentParser(
+        prog="taskqueue",
+        description="Distributed Task Queue CLI",
+    )
+    
+    subparsers = parser.add_subparsers(dest="command", help="Available commands")
+    
+    # Worker command
+    worker_parser = subparsers.add_parser("worker", help="Start a worker process")
+    worker_parser.add_argument(
+        "--name",
+        type=str,
+        default="worker",
+        help="Worker name",
+    )
+    worker_parser.add_argument(
+        "--concurrency",
+        type=int,
+        default=10,
+        help="Maximum concurrent jobs",
+    )
+    worker_parser.add_argument(
+        "--heartbeat-interval",
+        type=float,
+        default=10.0,
+        help="Heartbeat interval in seconds",
+    )
+    
+    # Submit command
+    submit_parser = subparsers.add_parser("submit", help="Submit a job")
+    submit_parser.add_argument("name", type=str, help="Job name")
+    submit_parser.add_argument(
+        "--payload",
+        type=str,
+        default="{}",
+        help="Job payload as JSON string",
+    )
+    submit_parser.add_argument(
+        "--priority",
+        type=str,
+        choices=["critical", "high", "normal", "low", "batch"],
+        default="normal",
+        help="Job priority",
+    )
+    submit_parser.add_argument(
+        "--delay",
+        type=int,
+        default=0,
+        help="Delay in milliseconds",
+    )
+    
+    # Status command
+    status_parser = subparsers.add_parser("status", help="Get queue status")
+    
+    # Inspect command
+    inspect_parser = subparsers.add_parser("inspect", help="Inspect a job")
+    inspect_parser.add_argument("job_id", type=str, help="Job ID to inspect")
+    
+    # Cancel command
+    cancel_parser = subparsers.add_parser("cancel", help="Cancel a job")
+    cancel_parser.add_argument("job_id", type=str, help="Job ID to cancel")
+    
+    return parser
+
+
+def priority_from_string(s: str) -> Priority:
+    """Convert string to Priority enum."""
+    mapping = {
+        "critical": Priority.CRITICAL,
+        "high": Priority.HIGH,
+        "normal": Priority.NORMAL,
+        "low": Priority.LOW,
+        "batch": Priority.BATCH,
+    }
+    return mapping.get(s.lower(), Priority.NORMAL)
+
+
+def run_worker(args: argparse.Namespace):
+    """Run a worker process."""
+    import json
+    
+    queue = TaskQueue()
+    
+    def default_handler(job: Job) -> JobResult:
+        """Default job handler that just logs the job."""
+        print(f"Processing job: {job.id} - {job.name}")
+        time.sleep(0.1)
+        return JobResult(
+            job_id=job.id,
+            success=True,
+            result={"processed": True},
+        )
+    
+    worker = WorkerProcess(
+        name=args.name,
+        handler=default_handler,
+        max_concurrent=args.concurrency,
+        heartbeat_interval=args.heartbeat_interval,
+    )
+    
+    queue.register_worker(worker)
+    worker.start()
+    
+    shutdown = GracefulShutdown(
+        worker.node,
+        queue._worker_registry,
+        on_job_reassign=lambda job: queue._priority_queue.enqueue(job),
+    )
+    
+    def signal_handler(signum, frame):
+        print("\nShutdown requested...")
+        shutdown.request_shutdown()
+    
+    signal.signal(signal.SIGINT, signal_handler)
+    signal.signal(signal.SIGTERM, signal_handler)
+    
+    print(f"Worker {worker.info.id} started ({args.name})")
+    print(f"Concurrency: {args.concurrency}")
+    
+    try:
+        while worker.is_running and not shutdown.is_shutdown_requested():
+            job = queue.get_next_job(timeout=1.0)
+            if job:
+                worker.node.assign_job(job)
+                
+                async def process():
+                    result = await worker.process_job(job)
+                    queue.complete_job(job.id, result)
+                    worker.node.complete_job(job.id, result.success)
+                
+                asyncio.run(process())
+            
+            queue.worker_heartbeat(worker.info.id)
+    
+    finally:
+        print("Executing graceful shutdown...")
+        unfinished = shutdown.execute_shutdown(timeout_seconds=30)
+        if unfinished:
+            print(f"Reassigned {len(unfinished)} unfinished jobs")
+        print("Worker stopped")
+
+
+def submit_job(args: argparse.Namespace):
+    """Submit a job via CLI."""
+    import json
+    
+    queue = TaskQueue()
+    
+    try:
+        payload = json.loads(args.payload)
+    except json.JSONDecodeError:
+        print(f"Error: Invalid JSON payload: {args.payload}")
+        sys.exit(1)
+    
+    priority = priority_from_string(args.priority)
+    
+    job_id = queue.submit(
+        name=args.name,
+        payload=payload,
+        priority=priority,
+        delay_ms=args.delay,
+    )
+    
+    print(f"Job submitted: {job_id}")
+
+
+def show_status(args: argparse.Namespace):
+    """Show queue status."""
+    queue = TaskQueue()
+    stats = queue.get_stats()
+    
+    print("Queue Status")
+    print("=" * 40)
+    print(f"Total Jobs:     {stats.total_jobs}")
+    print(f"Pending:        {stats.pending_jobs}")
+    print(f"Running:        {stats.running_jobs}")
+    print(f"Completed:      {stats.completed_jobs}")
+    print(f"Failed:         {stats.failed_jobs}")
+    print(f"Dead (DLQ):     {stats.dead_jobs}")
+    print(f"Avg Time (ms):  {stats.avg_processing_time_ms:.2f}")
+
+
+def inspect_job(args: argparse.Namespace):
+    """Inspect a specific job."""
+    queue = TaskQueue()
+    job = queue.get_job(args.job_id)
+    
+    if not job:
+        print(f"Job not found: {args.job_id}")
+        sys.exit(1)
+    
+    print(f"Job: {job.id}")
+    print("=" * 40)
+    print(f"Name:       {job.name}")
+    print(f"Status:     {job.status}")
+    print(f"Priority:   {Priority(job.priority).name}")
+    print(f"Attempt:    {job.attempt}")
+    print(f"Created:    {job.created_at}")
+    if job.started_at:
+        print(f"Started:    {job.started_at}")
+    if job.completed_at:
+        print(f"Completed:  {job.completed_at}")
+    if job.last_error:
+        print(f"Last Error: {job.last_error}")
+
+
+def cancel_job(args: argparse.Namespace):
+    """Cancel a job."""
+    queue = TaskQueue()
+    
+    if queue.cancel_job(args.job_id):
+        print(f"Job cancelled: {args.job_id}")
+    else:
+        print(f"Could not cancel job: {args.job_id}")
+        sys.exit(1)
+
+
+def main():
+    """Main CLI entry point."""
+    parser = create_parser()
+    args = parser.parse_args()
+    
+    if not args.command:
+        parser.print_help()
+        sys.exit(0)
+    
+    commands = {
+        "worker": run_worker,
+        "submit": submit_job,
+        "status": show_status,
+        "inspect": inspect_job,
+        "cancel": cancel_job,
+    }
+    
+    handler = commands.get(args.command)
+    if handler:
+        handler(args)
+    else:
+        parser.print_help()
+        sys.exit(1)
+
+
+if __name__ == "__main__":
+    main()
diff -ruN repository_before/client.py repository_after/client.py
--- repository_before/client.py	1970-01-01 03:00:00
+++ repository_after/client.py	2026-02-14 21:33:24
@@ -0,0 +1,487 @@
+"""Main task queue client interface.
+
+Uses Redis Streams for distributed queue, Prometheus client for metrics,
+and structlog for structured logging.
+"""
+from __future__ import annotations
+
+import asyncio
+import os
+import threading
+import time
+from datetime import datetime
+from typing import Any, Callable, Dict, List, Optional, TypeVar
+
+from .dependencies import CircularDependencyError, DependencyGraph, DependencyResolver
+from .models import Job, JobResult, JobStatus, Priority, QueueStats, RetryConfig
+from .prometheus_metrics import TaskQueuePrometheusMetrics, get_metrics
+from .redis_backend import RedisConfig, RedisConnection, RedisStreamsQueue
+from .retry import RetryDecision, RetryManager, RetryScheduler
+from .scheduler import (
+    BulkJobSubmitter,
+    DelayedJobScheduler,
+    RecurringJobScheduler,
+    UniquenessConstraint,
+)
+from .worker import WorkerNode, WorkerProcess, WorkerRegistry, WorkStealing
+from .logging_config import get_logger
+
+logger = get_logger(__name__)
+T = TypeVar("T")
+
+
+class TaskQueue:
+    """Main task queue client for submitting and managing jobs.
+    
+    Uses Redis Streams for distributed message delivery and Prometheus
+    client for metrics exposition.
+    """
+    
+    def __init__(
+        self,
+        redis_config: Optional[RedisConfig] = None,
+        heartbeat_timeout: float = 30.0,
+    ):
+        # Redis configuration with environment variable support
+        self._redis_config = redis_config or RedisConfig(
+            host=os.environ.get("REDIS_HOST", "localhost"),
+            port=int(os.environ.get("REDIS_PORT", 6379)),
+        )
+        
+        # Initialize Redis-based distributed queue
+        try:
+            self._redis_conn = RedisConnection.get_connection(self._redis_config)
+            self._queue = RedisStreamsQueue(self._redis_conn)
+            self._use_redis = True
+            logger.info("connected_to_redis", host=self._redis_config.host, port=self._redis_config.port)
+        except Exception as e:
+            logger.warning("redis_connection_failed", error=str(e), fallback="in-memory")
+            self._redis_conn = None
+            self._queue = None
+            self._use_redis = False
+        
+        # In-memory job storage for tracking
+        self._jobs: Dict[str, Job] = {}
+        self._jobs_lock = threading.RLock()
+        
+        self._delayed_scheduler = DelayedJobScheduler()
+        self._recurring_scheduler = RecurringJobScheduler()
+        self._dependency_resolver = DependencyResolver()
+        self._uniqueness = UniquenessConstraint()
+        
+        # Prometheus metrics
+        self._metrics = get_metrics()
+        
+        self._retry_manager = RetryManager(
+            on_retry=self._on_job_retry,
+            on_dlq=self._on_job_dlq,
+            on_failure=self._on_job_failure,
+        )
+        self._retry_scheduler = RetryScheduler(self._retry_manager)
+        
+        self._worker_registry = WorkerRegistry(heartbeat_timeout)
+        self._work_stealing = WorkStealing(self._worker_registry)
+        
+        self._bulk_submitter = BulkJobSubmitter(
+            self._delayed_scheduler,
+            on_submit=self._on_job_submitted,
+        )
+        
+        self._handlers: Dict[str, Callable[[Job], JobResult]] = {}
+        self._running = False
+        self._lock = threading.RLock()
+    
+    def _register_job(self, job: Job):
+        """Register a job in the local tracking store."""
+        with self._jobs_lock:
+            self._jobs[job.id] = job
+    
+    def _get_job(self, job_id: str) -> Optional[Job]:
+        """Get a job from local tracking store."""
+        with self._jobs_lock:
+            return self._jobs.get(job_id)
+    
+    def _enqueue(self, job: Job):
+        """Enqueue a job to Redis or in-memory fallback."""
+        if self._use_redis and self._queue:
+            self._queue.enqueue(job)
+        self._register_job(job)
+    
+    def _dequeue(self, timeout: Optional[float] = None) -> Optional[Job]:
+        """Dequeue a job from Redis or in-memory fallback."""
+        if self._use_redis and self._queue:
+            # Redis dequeue returns list of (message_id, job) tuples
+            timeout_ms = int(timeout * 1000) if timeout else 1000
+            results = self._queue.dequeue(
+                consumer_name=f"worker_{id(self)}",
+                timeout_ms=timeout_ms,
+                count=1,
+            )
+            if results:
+                message_id, job = results[0]
+                return job
+        return None
+    
+    def size(self) -> int:
+        """Get total queue size."""
+        if self._use_redis and self._queue:
+            return self._queue.size()
+        with self._jobs_lock:
+            return len([j for j in self._jobs.values() if j.status == JobStatus.PENDING])
+    
+    def size_by_priority(self) -> Dict[Priority, int]:
+        """Get queue size by priority level."""
+        if self._use_redis and self._queue:
+            return self._queue.size_by_priority()
+        result = {p: 0 for p in Priority}
+        with self._jobs_lock:
+            for job in self._jobs.values():
+                if job.status == JobStatus.PENDING:
+                    result[Priority(job.priority)] += 1
+        return result
+    
+    def register_handler(self, job_name: str, handler: Callable[[Job], JobResult]):
+        """Register a handler function for a job type."""
+        with self._lock:
+            self._handlers[job_name] = handler
+    
+    def submit(
+        self,
+        name: str,
+        payload: Dict[str, Any],
+        priority: Priority = Priority.NORMAL,
+        delay_ms: int = 0,
+        depends_on: Optional[List[str]] = None,
+        retry_config: Optional[RetryConfig] = None,
+        unique_key: Optional[str] = None,
+        cron_expression: Optional[str] = None,
+        timezone: str = "UTC",
+    ) -> str:
+        """Submit a new job to the queue."""
+        job = Job(
+            name=name,
+            payload=payload,
+            priority=priority,
+            delay_ms=delay_ms,
+            depends_on=depends_on or [],
+            retry_config=retry_config or RetryConfig(),
+            unique_key=unique_key,
+            cron_expression=cron_expression,
+            timezone=timezone,
+        )
+        
+        return self._submit_job(job)
+    
+    def _submit_job(self, job: Job) -> str:
+        """Internal job submission."""
+        if job.unique_key:
+            if not self._uniqueness.acquire(job.unique_key, job.id):
+                raise ValueError(f"Duplicate job with unique key: {job.unique_key}")
+        
+        if job.depends_on:
+            success, error = self._dependency_resolver.submit_job(job)
+            if not success:
+                if job.unique_key:
+                    self._uniqueness.release(job.unique_key)
+                raise CircularDependencyError([error or "Unknown dependency error"])
+        
+        if job.cron_expression:
+            self._recurring_scheduler.register(job)
+            self._register_job(job)
+            self._metrics.record_job_submitted(Priority(job.priority).name.lower())
+            return job.id
+        
+        if job.delay_ms > 0 or job.scheduled_at:
+            self._delayed_scheduler.schedule(job)
+            self._register_job(job)
+            self._metrics.record_job_submitted(Priority(job.priority).name.lower())
+            return job.id
+        
+        if job.depends_on and self._dependency_resolver.graph.has_unmet_dependencies(job.id):
+            job.status = JobStatus.PENDING
+            self._register_job(job)
+            self._metrics.record_job_submitted(Priority(job.priority).name.lower())
+            return job.id
+        
+        self._enqueue(job)
+        self._metrics.record_job_submitted(Priority(job.priority).name.lower())
+        self._update_metrics()
+        
+        return job.id
+    
+    def submit_batch(
+        self,
+        jobs: List[Dict[str, Any]],
+        atomic: bool = True,
+    ) -> tuple[List[str], List[tuple[str, str]]]:
+        """Submit multiple jobs with optional atomic semantics."""
+        job_objects = []
+        for job_data in jobs:
+            job = Job(
+                name=job_data["name"],
+                payload=job_data.get("payload", {}),
+                priority=job_data.get("priority", Priority.NORMAL),
+                delay_ms=job_data.get("delay_ms", 0),
+                depends_on=job_data.get("depends_on", []),
+                retry_config=job_data.get("retry_config", RetryConfig()),
+                unique_key=job_data.get("unique_key"),
+            )
+            job_objects.append(job)
+        
+        return self._bulk_submitter.submit_batch(job_objects, atomic)
+    
+    def get_job(self, job_id: str) -> Optional[Job]:
+        """Get a job by its ID."""
+        return self._get_job(job_id)
+    
+    def cancel_job(self, job_id: str) -> bool:
+        """Cancel a pending or scheduled job."""
+        job = self._get_job(job_id)
+        if not job:
+            return False
+        
+        if job.status == JobStatus.SCHEDULED:
+            self._delayed_scheduler.cancel(job_id)
+        elif job.status == JobStatus.PENDING:
+            if self._use_redis and self._queue:
+                self._queue.remove_job(job_id)
+        else:
+            return False
+        
+        if job.unique_key:
+            self._uniqueness.release(job.unique_key)
+        
+        job.status = JobStatus.FAILED
+        job.last_error = "Cancelled by user"
+        return True
+    
+    def update_priority(self, job_id: str, new_priority: Priority) -> bool:
+        """Update a job's priority."""
+        job = self._get_job(job_id)
+        if job:
+            job.priority = new_priority.value
+            return True
+        return False
+    
+    def get_next_job(self, timeout: Optional[float] = None) -> Optional[Job]:
+        """Get the next job to process from the queue."""
+        due_delayed = self._delayed_scheduler.get_due_jobs()
+        for job in due_delayed:
+            if not self._dependency_resolver.graph.has_unmet_dependencies(job.id):
+                return job
+            self._enqueue(job)
+        
+        due_recurring = self._recurring_scheduler.get_due_jobs()
+        for job in due_recurring:
+            new_job = Job(
+                name=job.name,
+                payload=job.payload,
+                priority=job.priority,
+            )
+            self._enqueue(new_job)
+        
+        due_retries = self._retry_scheduler.get_due_retries()
+        for job_id in due_retries:
+            self._retry_scheduler.pop_retry(job_id)
+            job = self._get_job(job_id)
+            if job:
+                job.status = JobStatus.PENDING
+                self._enqueue(job)
+        
+        job = self._dequeue(timeout)
+        if job:
+            self._update_metrics()
+        return job
+    
+    def complete_job(self, job_id: str, result: JobResult):
+        """Mark a job as complete."""
+        job = self._get_job(job_id)
+        if not job:
+            return
+        
+        if result.success:
+            job.status = JobStatus.COMPLETED
+            job.completed_at = datetime.utcnow()
+            
+            runnable = self._dependency_resolver.complete_job(job_id)
+            for dep_job in runnable:
+                self._enqueue(dep_job)
+            
+            self._metrics.record_job_completed(
+                result.duration_ms / 1000,
+                Priority(job.priority).name.lower(),
+            )
+        else:
+            decision = self._retry_manager.handle_failure(job, result.error or "Unknown error")
+            
+            if decision.should_retry:
+                self._retry_scheduler.schedule_retry(job, decision.delay_ms)
+            else:
+                self._dependency_resolver.fail_job(job_id)
+                self._metrics.record_job_failed(Priority(job.priority).name.lower())
+        
+        if job.unique_key and job.status in (JobStatus.COMPLETED, JobStatus.DEAD):
+            self._uniqueness.release(job.unique_key)
+        
+        self._update_metrics()
+    
+    def get_stats(self) -> QueueStats:
+        """Get queue statistics."""
+        return QueueStats(
+            total_jobs=len(self._jobs),
+            pending_jobs=len([j for j in self._jobs.values() if j.status == JobStatus.PENDING]),
+            completed_jobs=len([j for j in self._jobs.values() if j.status == JobStatus.COMPLETED]),
+            failed_jobs=len([j for j in self._jobs.values() if j.status == JobStatus.FAILED]),
+            queue_depth=self.size(),
+        )
+    
+    def get_dlq(self) -> List[Job]:
+        """Get jobs in the dead-letter queue."""
+        return self._retry_manager.get_dlq()
+    
+    def requeue_from_dlq(self, job_id: str, reset_attempts: bool = True) -> Optional[str]:
+        """Requeue a job from the dead-letter queue."""
+        job = self._retry_manager.requeue_from_dlq(job_id, reset_attempts)
+        if job:
+            self._enqueue(job)
+            self._update_metrics()
+            return job.id
+        return None
+    
+    def get_prometheus_metrics(self) -> str:
+        """Get metrics in Prometheus format."""
+        return self._metrics.export()
+    
+    def _on_job_submitted(self, job: Job):
+        """Callback when a job is submitted."""
+        logger.info("job_submitted", job_id=job.id, name=job.name)
+    
+    def _on_job_retry(self, job: Job, attempt: int, delay_ms: int):
+        """Callback when a job is scheduled for retry."""
+        logger.info("job_retry_scheduled", job_id=job.id, attempt=attempt, delay_ms=delay_ms)
+    
+    def _on_job_dlq(self, job: Job, reason: str):
+        """Callback when a job is sent to DLQ."""
+        logger.warning("job_sent_to_dlq", job_id=job.id, reason=reason)
+        self._metrics.record_dlq_added()
+    
+    def _on_job_failure(self, job: Job, error: str):
+        """Callback when a job fails permanently."""
+        logger.error("job_failed", job_id=job.id, error=error)
+    
+    def _update_metrics(self):
+        """Update queue metrics."""
+        size_by_priority = self.size_by_priority()
+        total_size = self.size()
+        self._metrics.set_queue_depth(total_size)
+        self._metrics.set_dlq_depth(self._retry_manager.get_dlq_size())
+        self._metrics.set_worker_count(self._worker_registry.get_worker_count())
+    
+    def register_worker(self, worker: WorkerProcess) -> bool:
+        """Register a worker with the queue."""
+        node = WorkerNode(info=worker.info)
+        return self._worker_registry.register(node)
+    
+    def unregister_worker(self, worker_id: str):
+        """Unregister a worker."""
+        self._worker_registry.unregister(worker_id)
+        self._update_metrics()
+    
+    def worker_heartbeat(self, worker_id: str) -> bool:
+        """Update worker heartbeat."""
+        return self._worker_registry.heartbeat(worker_id)
+    
+    def get_queue_size(self) -> int:
+        """Get total queue size."""
+        return self.size()
+    
+    def get_queue_size_by_priority(self) -> Dict[Priority, int]:
+        """Get queue size by priority level."""
+        return self.size_by_priority()
+    
+    def clear(self):
+        """Clear all jobs from the queue."""
+        if self._use_redis and self._queue:
+            self._queue.clear()
+        with self._jobs_lock:
+            self._jobs.clear()
+        self._update_metrics()
+    
+    def list_jobs(
+        self,
+        status: Optional[str] = None,
+        priority: Optional[str] = None,
+        limit: int = 50,
+        cursor: Optional[str] = None,
+    ) -> List[Job]:
+        """List jobs with optional filters."""
+        with self._jobs_lock:
+            jobs = list(self._jobs.values())
+            
+            jobs.sort(key=lambda j: j.created_at, reverse=True)
+            
+            if status:
+                target_status = status.upper()
+                jobs = [j for j in jobs if j.status.value.upper() == target_status]
+            if priority:
+                target_priority = priority.upper()
+                jobs = [j for j in jobs if Priority(j.priority).name == target_priority]
+            
+            if cursor:
+                start_idx = 0
+                for i, job in enumerate(jobs):
+                    if job.id == cursor:
+                        start_idx = i + 1
+                        break
+                jobs = jobs[start_idx:]
+            
+            return jobs[:limit]
+    
+    def get_queue_depths(self) -> Dict[Priority, int]:
+        """Get queue depths per priority level."""
+        return self.size_by_priority()
+    
+    def get_workers(self) -> List[WorkerNode]:
+        """Get all registered workers."""
+        return self._worker_registry.get_all_workers()
+    
+    def get_worker_count(self) -> int:
+        """Get number of registered workers."""
+        return self._worker_registry.get_worker_count()
+    
+    def get_dlq_jobs(self, limit: int = 50) -> List[Job]:
+        """Get jobs from dead-letter queue."""
+        return self._retry_manager.get_dlq()[:limit]
+    
+    def retry_job(self, job_id: str) -> Optional[Job]:
+        """Retry a failed job from DLQ."""
+        job = self._retry_manager.requeue_from_dlq(job_id, reset_attempts=True)
+        if job:
+            self._enqueue(job)
+            self._update_metrics()
+        return job
+
+
+class AsyncTaskQueue:
+    """Async wrapper for TaskQueue using Redis Streams."""
+    
+    def __init__(self, queue: Optional[TaskQueue] = None):
+        self._queue = queue or TaskQueue()
+    
+    async def submit(
+        self,
+        name: str,
+        payload: Dict[str, Any],
+        priority: Priority = Priority.NORMAL,
+        **kwargs,
+    ) -> str:
+        return self._queue.submit(name, payload, priority, **kwargs)
+    
+    async def get_next_job(self, timeout: Optional[float] = None) -> Optional[Job]:
+        return self._queue.get_next_job(timeout)
+    
+    async def complete_job(self, job_id: str, result: JobResult):
+        self._queue.complete_job(job_id, result)
+    
+    def get_stats(self) -> QueueStats:
+        return self._queue.get_stats()
diff -ruN repository_before/dependencies.py repository_after/dependencies.py
--- repository_before/dependencies.py	1970-01-01 03:00:00
+++ repository_after/dependencies.py	2026-02-14 21:33:24
@@ -0,0 +1,253 @@
+"""Dependency management system with topological sorting and cycle detection."""
+from __future__ import annotations
+
+from collections import defaultdict, deque
+from typing import Dict, List, Optional, Set, Tuple
+
+from .models import Job, JobStatus
+
+
+class CircularDependencyError(Exception):
+    """Raised when circular dependencies are detected."""
+    def __init__(self, cycle: List[str]):
+        self.cycle = cycle
+        super().__init__(f"Circular dependency detected: {' -> '.join(cycle)}")
+
+
+class DependencyGraph:
+    """Graph-based dependency management for jobs."""
+    
+    def __init__(self):
+        self._dependencies: Dict[str, Set[str]] = defaultdict(set)
+        self._dependents: Dict[str, Set[str]] = defaultdict(set)
+        self._job_status: Dict[str, JobStatus] = {}
+        self._jobs: Dict[str, Job] = {}
+    
+    def add_job(self, job: Job) -> None:
+        """Add a job and its dependencies to the graph."""
+        self._jobs[job.id] = job
+        self._job_status[job.id] = job.status
+        
+        for dep_id in job.depends_on:
+            self._dependencies[job.id].add(dep_id)
+            self._dependents[dep_id].add(job.id)
+    
+    def remove_job(self, job_id: str) -> None:
+        """Remove a job from the graph."""
+        for dep_id in self._dependencies.get(job_id, set()):
+            self._dependents[dep_id].discard(job_id)
+        
+        for dependent_id in self._dependents.get(job_id, set()):
+            self._dependencies[dependent_id].discard(job_id)
+        
+        self._dependencies.pop(job_id, None)
+        self._dependents.pop(job_id, None)
+        self._job_status.pop(job_id, None)
+        self._jobs.pop(job_id, None)
+    
+    def get_dependencies(self, job_id: str) -> Set[str]:
+        """Get all jobs that this job depends on."""
+        return self._dependencies.get(job_id, set()).copy()
+    
+    def get_dependents(self, job_id: str) -> Set[str]:
+        """Get all jobs that depend on this job."""
+        return self._dependents.get(job_id, set()).copy()
+    
+    def has_unmet_dependencies(self, job_id: str) -> bool:
+        """Check if job has any incomplete dependencies."""
+        for dep_id in self._dependencies.get(job_id, set()):
+            status = self._job_status.get(dep_id)
+            if status != JobStatus.COMPLETED:
+                return True
+        return False
+    
+    def get_unmet_dependencies(self, job_id: str) -> Set[str]:
+        """Get IDs of incomplete dependencies."""
+        unmet = set()
+        for dep_id in self._dependencies.get(job_id, set()):
+            status = self._job_status.get(dep_id)
+            if status != JobStatus.COMPLETED:
+                unmet.add(dep_id)
+        return unmet
+    
+    def mark_completed(self, job_id: str) -> List[str]:
+        """Mark job as completed and return newly runnable dependent jobs."""
+        self._job_status[job_id] = JobStatus.COMPLETED
+        
+        runnable = []
+        for dependent_id in self._dependents.get(job_id, set()):
+            if not self.has_unmet_dependencies(dependent_id):
+                runnable.append(dependent_id)
+        
+        return runnable
+    
+    def mark_failed(self, job_id: str) -> List[str]:
+        """Mark job as failed and return affected dependent jobs."""
+        self._job_status[job_id] = JobStatus.FAILED
+        return list(self._dependents.get(job_id, set()))
+    
+    def update_status(self, job_id: str, status: JobStatus) -> None:
+        """Update job status."""
+        self._job_status[job_id] = status
+    
+    def detect_cycle(self, job_id: str, new_dependencies: List[str]) -> Optional[List[str]]:
+        """Detect if adding dependencies would create a cycle."""
+        temp_deps = self._dependencies.copy()
+        temp_deps[job_id] = set(new_dependencies)
+        
+        visited = set()
+        rec_stack = set()
+        path = []
+        
+        def dfs(node: str) -> Optional[List[str]]:
+            visited.add(node)
+            rec_stack.add(node)
+            path.append(node)
+            
+            for neighbor in temp_deps.get(node, set()):
+                if neighbor not in visited:
+                    result = dfs(neighbor)
+                    if result:
+                        return result
+                elif neighbor in rec_stack:
+                    cycle_start = path.index(neighbor)
+                    return path[cycle_start:] + [neighbor]
+            
+            path.pop()
+            rec_stack.remove(node)
+            return None
+        
+        for node in list(temp_deps.keys()):
+            if node not in visited:
+                cycle = dfs(node)
+                if cycle:
+                    return cycle
+        
+        return None
+    
+    def validate_dependencies(self, job: Job) -> None:
+        """Validate job dependencies, raising error if invalid."""
+        if not job.depends_on:
+            return
+        
+        cycle = self.detect_cycle(job.id, job.depends_on)
+        if cycle:
+            raise CircularDependencyError(cycle)
+    
+    def topological_sort(self) -> List[str]:
+        """Return jobs in topological order (dependencies first)."""
+        in_degree = defaultdict(int)
+        
+        for job_id in self._jobs:
+            if job_id not in in_degree:
+                in_degree[job_id] = 0
+            for dep_id in self._dependencies.get(job_id, set()):
+                in_degree[job_id] += 1
+        
+        queue = deque([job_id for job_id, degree in in_degree.items() if degree == 0])
+        result = []
+        
+        while queue:
+            job_id = queue.popleft()
+            result.append(job_id)
+            
+            for dependent_id in self._dependents.get(job_id, set()):
+                in_degree[dependent_id] -= 1
+                if in_degree[dependent_id] == 0:
+                    queue.append(dependent_id)
+        
+        if len(result) != len(self._jobs):
+            raise CircularDependencyError(["cycle detected in graph"])
+        
+        return result
+    
+    def get_ready_jobs(self) -> List[str]:
+        """Get all jobs that are ready to run (no unmet dependencies)."""
+        ready = []
+        for job_id, status in self._job_status.items():
+            if status == JobStatus.PENDING and not self.has_unmet_dependencies(job_id):
+                ready.append(job_id)
+        return ready
+    
+    def get_job(self, job_id: str) -> Optional[Job]:
+        """Get job by ID."""
+        return self._jobs.get(job_id)
+    
+    def get_all_jobs(self) -> List[Job]:
+        """Get all jobs in the graph."""
+        return list(self._jobs.values())
+    
+    def clear(self) -> None:
+        """Clear all jobs from the graph."""
+        self._dependencies.clear()
+        self._dependents.clear()
+        self._job_status.clear()
+        self._jobs.clear()
+
+
+class DependencyResolver:
+    """High-level dependency resolution with batch operations."""
+    
+    def __init__(self, graph: Optional[DependencyGraph] = None):
+        self._graph = graph or DependencyGraph()
+    
+    @property
+    def graph(self) -> DependencyGraph:
+        return self._graph
+    
+    def submit_job(self, job: Job) -> Tuple[bool, Optional[str]]:
+        """Submit a job with dependency validation."""
+        try:
+            self._graph.validate_dependencies(job)
+            self._graph.add_job(job)
+            return True, None
+        except CircularDependencyError as e:
+            return False, str(e)
+    
+    def submit_batch(self, jobs: List[Job]) -> Tuple[List[str], List[Tuple[str, str]]]:
+        """Submit multiple jobs, returning (successful_ids, failed_pairs)."""
+        temp_graph = DependencyGraph()
+        
+        for job in jobs:
+            temp_graph.add_job(job)
+        
+        try:
+            temp_graph.topological_sort()
+        except CircularDependencyError as e:
+            return [], [(j.id, str(e)) for j in jobs]
+        
+        successful = []
+        failed = []
+        
+        for job in jobs:
+            success, error = self.submit_job(job)
+            if success:
+                successful.append(job.id)
+            else:
+                failed.append((job.id, error))
+        
+        return successful, failed
+    
+    def complete_job(self, job_id: str) -> List[Job]:
+        """Mark job completed and return newly runnable jobs."""
+        runnable_ids = self._graph.mark_completed(job_id)
+        return [self._graph.get_job(jid) for jid in runnable_ids if self._graph.get_job(jid)]
+    
+    def fail_job(self, job_id: str) -> List[Job]:
+        """Mark job failed and return affected dependent jobs."""
+        affected_ids = self._graph.mark_failed(job_id)
+        return [self._graph.get_job(jid) for jid in affected_ids if self._graph.get_job(jid)]
+    
+    def get_execution_order(self) -> List[Job]:
+        """Get jobs in dependency-respecting execution order."""
+        order = self._graph.topological_sort()
+        return [self._graph.get_job(jid) for jid in order if self._graph.get_job(jid)]
+    
+    def is_ready(self, job_id: str) -> bool:
+        """Check if job is ready to execute."""
+        return not self._graph.has_unmet_dependencies(job_id)
+    
+    def get_ready_jobs(self) -> List[Job]:
+        """Get all jobs ready for execution."""
+        ready_ids = self._graph.get_ready_jobs()
+        return [self._graph.get_job(jid) for jid in ready_ids if self._graph.get_job(jid)]
diff -ruN repository_before/logging_config.py repository_after/logging_config.py
--- repository_before/logging_config.py	1970-01-01 03:00:00
+++ repository_after/logging_config.py	2026-02-14 21:33:24
@@ -0,0 +1,79 @@
+"""Structured logging configuration using structlog."""
+from __future__ import annotations
+
+import logging
+import sys
+from typing import Any, Dict, Optional
+
+import structlog
+from structlog.types import Processor
+
+
+def configure_logging(
+    level: str = "INFO",
+    json_format: bool = False,
+    add_timestamp: bool = True,
+) -> None:
+    """Configure structlog for the application."""
+    
+    timestamper = structlog.processors.TimeStamper(fmt="iso")
+    
+    shared_processors: list[Processor] = [
+        structlog.contextvars.merge_contextvars,
+        structlog.stdlib.add_log_level,
+        structlog.stdlib.add_logger_name,
+        structlog.stdlib.PositionalArgumentsFormatter(),
+        timestamper,
+        structlog.processors.StackInfoRenderer(),
+        structlog.processors.UnicodeDecoder(),
+    ]
+    
+    if json_format:
+        renderer = structlog.processors.JSONRenderer()
+    else:
+        renderer = structlog.dev.ConsoleRenderer(colors=True)
+    
+    structlog.configure(
+        processors=shared_processors + [
+            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
+        ],
+        logger_factory=structlog.stdlib.LoggerFactory(),
+        wrapper_class=structlog.stdlib.BoundLogger,
+        cache_logger_on_first_use=True,
+    )
+    
+    formatter = structlog.stdlib.ProcessorFormatter(
+        foreign_pre_chain=shared_processors,
+        processors=[
+            structlog.stdlib.ProcessorFormatter.remove_processors_meta,
+            renderer,
+        ],
+    )
+    
+    handler = logging.StreamHandler(sys.stdout)
+    handler.setFormatter(formatter)
+    
+    root_logger = logging.getLogger()
+    root_logger.handlers = [handler]
+    root_logger.setLevel(getattr(logging, level.upper()))
+    
+    logging.getLogger("uvicorn").handlers = []
+    logging.getLogger("uvicorn.access").handlers = []
+
+
+def get_logger(name: str) -> structlog.stdlib.BoundLogger:
+    """Get a configured logger instance."""
+    return structlog.get_logger(name)
+
+
+def bind_context(**kwargs: Any) -> None:
+    """Bind context variables to all subsequent log calls."""
+    structlog.contextvars.bind_contextvars(**kwargs)
+
+
+def clear_context() -> None:
+    """Clear all bound context variables."""
+    structlog.contextvars.clear_contextvars()
+
+
+configure_logging()
diff -ruN repository_before/models.py repository_after/models.py
--- repository_before/models.py	1970-01-01 03:00:00
+++ repository_after/models.py	2026-02-14 21:33:24
@@ -0,0 +1,174 @@
+"""Pydantic models for job definitions and payloads."""
+from __future__ import annotations
+
+import uuid
+from datetime import datetime
+from enum import Enum
+from typing import Any, Dict, Generic, List, Optional, TypeVar
+
+from pydantic import BaseModel, Field, field_validator
+
+
+class Priority(int, Enum):
+    """Job priority levels (lower value = higher priority)."""
+    CRITICAL = 0
+    HIGH = 1
+    NORMAL = 2
+    LOW = 3
+    BATCH = 4
+
+
+class JobStatus(str, Enum):
+    """Job lifecycle states."""
+    PENDING = "pending"
+    SCHEDULED = "scheduled"
+    RUNNING = "running"
+    COMPLETED = "completed"
+    FAILED = "failed"
+    RETRYING = "retrying"
+    DEAD = "dead"  # In dead-letter queue
+
+
+class RetryStrategy(str, Enum):
+    """Retry strategy types."""
+    FIXED = "fixed"
+    EXPONENTIAL = "exponential"
+    CUSTOM = "custom"
+
+
+T = TypeVar("T")
+
+
+class RetryConfig(BaseModel):
+    """Configuration for job retry behavior."""
+    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL
+    max_attempts: int = Field(default=3, ge=1)
+    base_delay_ms: int = Field(default=1000, ge=0)
+    max_delay_ms: int = Field(default=60000, ge=0)
+    jitter: bool = True
+    custom_delays_ms: Optional[List[int]] = None
+
+    @field_validator("custom_delays_ms")
+    @classmethod
+    def validate_custom_delays(cls, v, info):
+        if info.data.get("strategy") == RetryStrategy.CUSTOM and not v:
+            raise ValueError("custom_delays_ms required for custom strategy")
+        return v
+
+
+class JobPayload(BaseModel, Generic[T]):
+    """Generic job payload wrapper with versioning."""
+    version: int = 1
+    data: T
+    metadata: Dict[str, Any] = Field(default_factory=dict)
+
+
+PayloadT = TypeVar("PayloadT", bound=BaseModel)
+
+
+class TypedJob(BaseModel, Generic[PayloadT]):
+    """Type-safe job model with generic payload."""
+    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
+    name: str
+    payload: PayloadT
+    priority: Priority = Priority.NORMAL
+    status: JobStatus = JobStatus.PENDING
+    
+    scheduled_at: Optional[datetime] = None
+    delay_ms: int = 0
+    cron_expression: Optional[str] = None
+    timezone: str = "UTC"
+    depends_on: List[str] = Field(default_factory=list)
+    dependent_jobs: List[str] = Field(default_factory=list)
+    retry_config: "RetryConfig" = Field(default_factory=lambda: RetryConfig())
+    attempt: int = 0
+    last_error: Optional[str] = None
+    unique_key: Optional[str] = None
+    created_at: datetime = Field(default_factory=datetime.utcnow)
+    started_at: Optional[datetime] = None
+    completed_at: Optional[datetime] = None
+    worker_id: Optional[str] = None
+    on_success: Optional[str] = None
+    on_failure: Optional[str] = None
+    
+    model_config = {"use_enum_values": True}
+
+
+class Job(BaseModel):
+    """Core job model representing a task in the queue."""
+    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
+    name: str
+    payload: Dict[str, Any]
+    priority: Priority = Priority.NORMAL
+    status: JobStatus = JobStatus.PENDING
+    
+    # Scheduling
+    scheduled_at: Optional[datetime] = None
+    delay_ms: int = 0
+    cron_expression: Optional[str] = None
+    timezone: str = "UTC"
+    
+    # Dependencies
+    depends_on: List[str] = Field(default_factory=list)
+    dependent_jobs: List[str] = Field(default_factory=list)
+    
+    # Retry configuration
+    retry_config: RetryConfig = Field(default_factory=RetryConfig)
+    attempt: int = 0
+    last_error: Optional[str] = None
+    
+    # Uniqueness
+    unique_key: Optional[str] = None
+    
+    # Timestamps
+    created_at: datetime = Field(default_factory=datetime.utcnow)
+    started_at: Optional[datetime] = None
+    completed_at: Optional[datetime] = None
+    
+    # Worker assignment
+    worker_id: Optional[str] = None
+    
+    # Callbacks
+    on_success: Optional[str] = None
+    on_failure: Optional[str] = None
+    
+    class Config:
+        use_enum_values = True
+
+
+class JobResult(BaseModel):
+    """Result of job execution."""
+    job_id: str
+    success: bool
+    result: Optional[Any] = None
+    error: Optional[str] = None
+    duration_ms: float = 0
+    completed_at: datetime = Field(default_factory=datetime.utcnow)
+
+
+class WorkerInfo(BaseModel):
+    """Worker node information."""
+    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
+    name: str
+    host: str
+    port: int
+    status: str = "active"
+    last_heartbeat: datetime = Field(default_factory=datetime.utcnow)
+    current_jobs: List[str] = Field(default_factory=list)
+    max_concurrent_jobs: int = 10
+    processed_count: int = 0
+    failed_count: int = 0
+    is_leader: bool = False
+
+
+class QueueStats(BaseModel):
+    """Queue statistics for monitoring."""
+    total_jobs: int = 0
+    pending_jobs: int = 0
+    running_jobs: int = 0
+    completed_jobs: int = 0
+    failed_jobs: int = 0
+    dead_jobs: int = 0
+    jobs_by_priority: Dict[str, int] = Field(default_factory=dict)
+    avg_processing_time_ms: float = 0
+    throughput_per_second: float = 0
diff -ruN repository_before/multiprocess_worker.py repository_after/multiprocess_worker.py
--- repository_before/multiprocess_worker.py	1970-01-01 03:00:00
+++ repository_after/multiprocess_worker.py	2026-02-14 21:33:24
@@ -0,0 +1,319 @@
+"""Multiprocessing worker for CPU-bound tasks."""
+from __future__ import annotations
+
+import asyncio
+import multiprocessing as mp
+import os
+import signal
+import time
+from concurrent.futures import ProcessPoolExecutor, Future
+from dataclasses import dataclass
+from multiprocessing import Queue, Process
+from typing import Any, Callable, Dict, List, Optional, Tuple
+
+from .logging_config import get_logger
+from .models import Job, JobResult, JobStatus, WorkerInfo
+from .prometheus_metrics import get_metrics
+
+logger = get_logger(__name__)
+
+
+@dataclass
+class WorkerTask:
+    """Task to be executed by worker process."""
+    job_id: str
+    job_name: str
+    payload: Dict[str, Any]
+    handler_name: str
+
+
+@dataclass
+class WorkerResult:
+    """Result from worker process."""
+    job_id: str
+    success: bool
+    result: Optional[Any] = None
+    error: Optional[str] = None
+    duration_seconds: float = 0.0
+
+
+def _worker_process_entry(
+    task_queue: Queue,
+    result_queue: Queue,
+    handlers: Dict[str, Callable],
+    worker_id: str,
+):
+    """Entry point for worker subprocess."""
+    import signal
+    
+    signal.signal(signal.SIGINT, signal.SIG_IGN)
+    
+    logger.info("worker_process_started", worker_id=worker_id, pid=os.getpid())
+    
+    while True:
+        try:
+            task: Optional[WorkerTask] = task_queue.get(timeout=1.0)
+            
+            if task is None:
+                logger.info("worker_process_shutdown", worker_id=worker_id)
+                break
+            
+            handler = handlers.get(task.handler_name)
+            if not handler:
+                result_queue.put(WorkerResult(
+                    job_id=task.job_id,
+                    success=False,
+                    error=f"Handler not found: {task.handler_name}",
+                ))
+                continue
+            
+            start_time = time.time()
+            try:
+                result = handler(task.payload)
+                duration = time.time() - start_time
+                
+                result_queue.put(WorkerResult(
+                    job_id=task.job_id,
+                    success=True,
+                    result=result,
+                    duration_seconds=duration,
+                ))
+                
+            except Exception as e:
+                duration = time.time() - start_time
+                result_queue.put(WorkerResult(
+                    job_id=task.job_id,
+                    success=False,
+                    error=str(e),
+                    duration_seconds=duration,
+                ))
+                
+        except Exception:
+            continue
+
+
+class MultiprocessWorkerPool:
+    """Pool of worker processes for CPU-bound tasks."""
+    
+    def __init__(
+        self,
+        num_workers: int = None,
+        handlers: Optional[Dict[str, Callable]] = None,
+    ):
+        self._num_workers = num_workers or mp.cpu_count()
+        self._handlers = handlers or {}
+        self._task_queue: Queue = mp.Queue()
+        self._result_queue: Queue = mp.Queue()
+        self._processes: List[Process] = []
+        self._running = False
+        self._metrics = get_metrics()
+    
+    def register_handler(self, name: str, handler: Callable):
+        """Register a job handler."""
+        self._handlers[name] = handler
+        logger.info("handler_registered", handler_name=name)
+    
+    def start(self):
+        """Start worker processes."""
+        if self._running:
+            return
+        
+        self._running = True
+        
+        for i in range(self._num_workers):
+            worker_id = f"worker-{i}"
+            p = Process(
+                target=_worker_process_entry,
+                args=(
+                    self._task_queue,
+                    self._result_queue,
+                    self._handlers,
+                    worker_id,
+                ),
+                daemon=True,
+            )
+            p.start()
+            self._processes.append(p)
+            logger.info("worker_process_spawned", worker_id=worker_id, pid=p.pid)
+        
+        self._metrics.update_worker_count(len(self._processes))
+    
+    def stop(self, timeout: float = 5.0):
+        """Stop all worker processes gracefully."""
+        if not self._running:
+            return
+        
+        self._running = False
+        
+        for _ in self._processes:
+            self._task_queue.put(None)
+        
+        for p in self._processes:
+            p.join(timeout=timeout)
+            if p.is_alive():
+                logger.warning("worker_process_forceful_kill", pid=p.pid)
+                p.terminate()
+        
+        self._processes.clear()
+        self._metrics.update_worker_count(0)
+        logger.info("worker_pool_stopped")
+    
+    def submit(self, job: Job, handler_name: str) -> bool:
+        """Submit a job for processing."""
+        if not self._running:
+            return False
+        
+        task = WorkerTask(
+            job_id=job.id,
+            job_name=job.name,
+            payload=job.payload,
+            handler_name=handler_name,
+        )
+        
+        self._task_queue.put(task)
+        logger.debug("job_submitted_to_pool", job_id=job.id)
+        return True
+    
+    def get_results(self, timeout: float = 0.1) -> List[WorkerResult]:
+        """Get completed job results."""
+        results = []
+        
+        while True:
+            try:
+                result = self._result_queue.get(timeout=timeout)
+                results.append(result)
+            except Exception:
+                break
+        
+        return results
+    
+    @property
+    def worker_count(self) -> int:
+        return len(self._processes)
+    
+    @property
+    def is_running(self) -> bool:
+        return self._running
+
+
+class AsyncWorkerPool:
+    """Async wrapper for I/O-bound tasks using asyncio."""
+    
+    def __init__(self, max_concurrent: int = 100):
+        self._max_concurrent = max_concurrent
+        self._semaphore = asyncio.Semaphore(max_concurrent)
+        self._handlers: Dict[str, Callable] = {}
+        self._running_jobs: Dict[str, asyncio.Task] = {}
+        self._metrics = get_metrics()
+    
+    def register_handler(self, name: str, handler: Callable):
+        """Register an async job handler."""
+        self._handlers[name] = handler
+    
+    async def submit(self, job: Job, handler_name: str) -> Optional[asyncio.Task]:
+        """Submit an async job for processing."""
+        handler = self._handlers.get(handler_name)
+        if not handler:
+            logger.error("handler_not_found", handler_name=handler_name)
+            return None
+        
+        async def run_job():
+            async with self._semaphore:
+                start_time = time.time()
+                try:
+                    if asyncio.iscoroutinefunction(handler):
+                        result = await handler(job.payload)
+                    else:
+                        result = handler(job.payload)
+                    
+                    duration = time.time() - start_time
+                    return WorkerResult(
+                        job_id=job.id,
+                        success=True,
+                        result=result,
+                        duration_seconds=duration,
+                    )
+                except Exception as e:
+                    duration = time.time() - start_time
+                    return WorkerResult(
+                        job_id=job.id,
+                        success=False,
+                        error=str(e),
+                        duration_seconds=duration,
+                    )
+                finally:
+                    self._running_jobs.pop(job.id, None)
+        
+        task = asyncio.create_task(run_job())
+        self._running_jobs[job.id] = task
+        return task
+    
+    async def wait_all(self, timeout: Optional[float] = None) -> List[WorkerResult]:
+        """Wait for all running jobs to complete."""
+        if not self._running_jobs:
+            return []
+        
+        tasks = list(self._running_jobs.values())
+        done, pending = await asyncio.wait(
+            tasks,
+            timeout=timeout,
+            return_when=asyncio.ALL_COMPLETED,
+        )
+        
+        results = []
+        for task in done:
+            try:
+                result = task.result()
+                if result:
+                    results.append(result)
+            except Exception:
+                pass
+        
+        return results
+    
+    @property
+    def running_count(self) -> int:
+        return len(self._running_jobs)
+
+
+class HybridWorkerPool:
+    """Hybrid worker pool combining multiprocessing and asyncio."""
+    
+    def __init__(
+        self,
+        cpu_workers: int = None,
+        io_concurrency: int = 100,
+    ):
+        self._cpu_pool = MultiprocessWorkerPool(num_workers=cpu_workers)
+        self._io_pool = AsyncWorkerPool(max_concurrent=io_concurrency)
+        self._cpu_handlers: set = set()
+        self._io_handlers: set = set()
+    
+    def register_cpu_handler(self, name: str, handler: Callable):
+        """Register a CPU-bound job handler."""
+        self._cpu_pool.register_handler(name, handler)
+        self._cpu_handlers.add(name)
+    
+    def register_io_handler(self, name: str, handler: Callable):
+        """Register an I/O-bound job handler."""
+        self._io_pool.register_handler(name, handler)
+        self._io_handlers.add(name)
+    
+    def start(self):
+        """Start the CPU worker pool."""
+        self._cpu_pool.start()
+    
+    def stop(self, timeout: float = 5.0):
+        """Stop all workers."""
+        self._cpu_pool.stop(timeout)
+    
+    async def submit(self, job: Job, handler_name: str) -> bool:
+        """Submit a job to the appropriate pool."""
+        if handler_name in self._cpu_handlers:
+            return self._cpu_pool.submit(job, handler_name)
+        elif handler_name in self._io_handlers:
+            task = await self._io_pool.submit(job, handler_name)
+            return task is not None
+        else:
+            logger.error("unknown_handler", handler_name=handler_name)
+            return False
diff -ruN repository_before/prometheus_metrics.py repository_after/prometheus_metrics.py
--- repository_before/prometheus_metrics.py	1970-01-01 03:00:00
+++ repository_after/prometheus_metrics.py	2026-02-14 21:33:24
@@ -0,0 +1,225 @@
+"""Prometheus metrics using the official prometheus-client library."""
+from __future__ import annotations
+
+from typing import Dict, Optional
+
+from prometheus_client import (
+    REGISTRY,
+    Counter,
+    Gauge,
+    Histogram,
+    Info,
+    generate_latest,
+    CollectorRegistry,
+)
+
+from .models import Priority
+
+
+class TaskQueuePrometheusMetrics:
+    """Prometheus metrics for the task queue system."""
+    
+    def __init__(self, registry: Optional[CollectorRegistry] = None):
+        self._registry = registry or REGISTRY
+        
+        self.jobs_submitted = Counter(
+            "taskqueue_jobs_submitted_total",
+            "Total number of jobs submitted",
+            ["priority"],
+            registry=self._registry,
+        )
+        
+        self.jobs_completed = Counter(
+            "taskqueue_jobs_completed_total",
+            "Total number of jobs completed successfully",
+            ["priority"],
+            registry=self._registry,
+        )
+        
+        self.jobs_failed = Counter(
+            "taskqueue_jobs_failed_total",
+            "Total number of jobs that failed",
+            ["priority", "error_type"],
+            registry=self._registry,
+        )
+        
+        self.jobs_retried = Counter(
+            "taskqueue_jobs_retried_total",
+            "Total number of job retries",
+            ["priority"],
+            registry=self._registry,
+        )
+        
+        self.jobs_dead_lettered = Counter(
+            "taskqueue_jobs_dead_lettered_total",
+            "Total number of jobs sent to dead letter queue",
+            ["priority"],
+            registry=self._registry,
+        )
+        
+        self.queue_depth = Gauge(
+            "taskqueue_queue_depth",
+            "Current queue depth",
+            ["priority"],
+            registry=self._registry,
+        )
+        
+        self.running_jobs = Gauge(
+            "taskqueue_running_jobs",
+            "Number of currently running jobs",
+            ["worker_id"],
+            registry=self._registry,
+        )
+        
+        self.worker_count = Gauge(
+            "taskqueue_worker_count",
+            "Number of active workers",
+            registry=self._registry,
+        )
+        
+        self.job_processing_duration = Histogram(
+            "taskqueue_job_processing_duration_seconds",
+            "Job processing duration in seconds",
+            ["priority", "job_name"],
+            buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
+            registry=self._registry,
+        )
+        
+        self.job_wait_duration = Histogram(
+            "taskqueue_job_wait_duration_seconds",
+            "Time jobs spend waiting in queue",
+            ["priority"],
+            buckets=(0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0, 300.0),
+            registry=self._registry,
+        )
+        
+        self.throughput = Gauge(
+            "taskqueue_throughput_jobs_per_second",
+            "Current job throughput (jobs/sec)",
+            registry=self._registry,
+        )
+        
+        self.dlq_depth = Gauge(
+            "taskqueue_dlq_depth",
+            "Number of jobs in dead letter queue",
+            registry=self._registry,
+        )
+        
+        self.worker_info = Info(
+            "taskqueue_worker",
+            "Worker information",
+            registry=self._registry,
+        )
+        
+        self._jobs_completed_count = 0
+        self._jobs_completed_window_start = 0
+    
+    def record_job_submitted(self, priority):
+        """Record a job submission."""
+        priority_name = priority if isinstance(priority, str) else priority.name
+        self.jobs_submitted.labels(priority=priority_name).inc()
+    
+    def record_job_completed(
+        self, 
+        duration_seconds: float,
+        priority = "normal",
+        job_name: str = "unknown",
+        wait_seconds: float = 0,
+    ):
+        """Record successful job completion."""
+        priority_name = priority if isinstance(priority, str) else priority.name
+        self.jobs_completed.labels(priority=priority_name).inc()
+        self.job_processing_duration.labels(
+            priority=priority_name, 
+            job_name=job_name,
+        ).observe(duration_seconds)
+        
+        if wait_seconds > 0:
+            self.job_wait_duration.labels(priority=priority_name).observe(wait_seconds)
+        
+        self._update_throughput()
+    
+    def record_job_failed(
+        self, 
+        priority = "normal",
+        error_type: str = "unknown",
+    ):
+        """Record job failure."""
+        priority_name = priority if isinstance(priority, str) else priority.name
+        self.jobs_failed.labels(priority=priority_name, error_type=error_type).inc()
+    
+    def record_job_retried(self, priority = "normal"):
+        """Record job retry."""
+        priority_name = priority if isinstance(priority, str) else priority.name
+        self.jobs_retried.labels(priority=priority_name).inc()
+    
+    def record_job_dead_lettered(self, priority = "normal"):
+        """Record job sent to DLQ."""
+        priority_name = priority if isinstance(priority, str) else priority.name
+        self.jobs_dead_lettered.labels(priority=priority_name).inc()
+    
+    def record_dlq_added(self):
+        """Record job added to DLQ (simplified)."""
+        self.dlq_depth.inc()
+    
+    def set_queue_depth(self, depth: int):
+        """Set total queue depth."""
+        self.queue_depth.labels(priority="total").set(depth)
+    
+    def set_dlq_depth(self, depth: int):
+        """Set DLQ depth."""
+        self.dlq_depth.set(depth)
+    
+    def set_worker_count(self, count: int):
+        """Set worker count."""
+        self.worker_count.set(count)
+    
+    def update_queue_depth(self, depths):
+        """Update queue depth gauges."""
+        for priority, depth in depths.items():
+            priority_name = priority if isinstance(priority, str) else priority.name
+            self.queue_depth.labels(priority=priority_name).set(depth)
+    
+    def update_running_jobs(self, worker_id: str, count: int):
+        """Update running jobs count for a worker."""
+        self.running_jobs.labels(worker_id=worker_id).set(count)
+    
+    def update_worker_count(self, count: int):
+        """Update active worker count."""
+        self.worker_count.set(count)
+    
+    def set_worker_info(self, info: Dict[str, str]):
+        """Set worker information."""
+        self.worker_info.info(info)
+    
+    def _update_throughput(self):
+        """Update throughput calculation."""
+        import time
+        current_time = time.time()
+        
+        if self._jobs_completed_window_start == 0:
+            self._jobs_completed_window_start = current_time
+        
+        self._jobs_completed_count += 1
+        
+        window_duration = current_time - self._jobs_completed_window_start
+        if window_duration >= 1.0:
+            throughput = self._jobs_completed_count / window_duration
+            self.throughput.set(throughput)
+            self._jobs_completed_count = 0
+            self._jobs_completed_window_start = current_time
+    
+    def export(self) -> bytes:
+        """Export metrics in Prometheus format."""
+        return generate_latest(self._registry)
+
+
+_metrics_instance: Optional[TaskQueuePrometheusMetrics] = None
+
+
+def get_metrics() -> TaskQueuePrometheusMetrics:
+    """Get global metrics instance."""
+    global _metrics_instance
+    if _metrics_instance is None:
+        _metrics_instance = TaskQueuePrometheusMetrics()
+    return _metrics_instance
diff -ruN repository_before/pyproject.toml repository_after/pyproject.toml
--- repository_before/pyproject.toml	1970-01-01 03:00:00
+++ repository_after/pyproject.toml	2026-02-14 21:33:24
@@ -0,0 +1,79 @@
+[build-system]
+requires = ["setuptools>=61.0", "wheel"]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "taskqueue"
+version = "1.0.0"
+description = "High-performance distributed task queue system with Redis Streams"
+readme = "README.md"
+license = {text = "MIT"}
+requires-python = ">=3.11"
+authors = [
+    {name = "Task Queue Team", email = "taskqueue@example.com"}
+]
+classifiers = [
+    "Development Status :: 4 - Beta",
+    "Intended Audience :: Developers",
+    "License :: OSI Approved :: MIT License",
+    "Programming Language :: Python :: 3",
+    "Programming Language :: Python :: 3.11",
+    "Programming Language :: Python :: 3.12",
+    "Topic :: Software Development :: Libraries :: Python Modules",
+    "Topic :: System :: Distributed Computing",
+]
+keywords = ["task-queue", "distributed", "redis", "asyncio", "workers"]
+
+dependencies = [
+    "redis>=5.0.0",
+    "pydantic>=2.0.0",
+    "structlog>=24.0.0",
+    "prometheus-client>=0.19.0",
+    "fastapi>=0.109.0",
+    "uvicorn>=0.27.0",
+    "msgpack>=1.0.0",
+]
+
+[project.optional-dependencies]
+dev = [
+    "pytest>=7.0.0",
+    "pytest-asyncio>=0.21.0",
+    "httpx>=0.26.0",
+    "black>=23.0.0",
+    "mypy>=1.0.0",
+    "ruff>=0.1.0",
+]
+
+[project.scripts]
+taskqueue = "taskqueue.cli:main"
+taskqueue-worker = "taskqueue.cli:run_worker"
+taskqueue-api = "taskqueue.cli:run_api"
+
+[project.urls]
+Homepage = "https://github.com/example/taskqueue"
+Documentation = "https://github.com/example/taskqueue#readme"
+Repository = "https://github.com/example/taskqueue"
+
+[tool.setuptools.packages.find]
+where = ["."]
+include = ["taskqueue*"]
+
+[tool.setuptools.package-dir]
+taskqueue = "."
+
+[tool.pytest.ini_options]
+asyncio_mode = "auto"
+testpaths = ["tests"]
+
+[tool.black]
+line-length = 100
+target-version = ["py311"]
+
+[tool.ruff]
+line-length = 100
+target-version = "py311"
+
+[tool.mypy]
+python_version = "3.11"
+warn_return_any = true
+warn_unused_configs = true
diff -ruN repository_before/redis_backend.py repository_after/redis_backend.py
--- repository_before/redis_backend.py	1970-01-01 03:00:00
+++ repository_after/redis_backend.py	2026-02-14 21:33:24
@@ -0,0 +1,391 @@
+"""Redis backend for distributed task queue using Redis Streams."""
+from __future__ import annotations
+
+import json
+import time
+from datetime import datetime
+from typing import Any, Dict, List, Optional, Tuple
+
+import redis
+from redis import Redis
+from redis.exceptions import RedisError
+
+from .logging_config import get_logger
+from .models import Job, JobStatus, Priority
+
+logger = get_logger(__name__)
+
+
+class RedisConfig:
+    """Redis connection configuration."""
+    
+    def __init__(
+        self,
+        host: str = "localhost",
+        port: int = 6379,
+        db: int = 0,
+        password: Optional[str] = None,
+        decode_responses: bool = True,
+        socket_timeout: float = 5.0,
+        socket_connect_timeout: float = 5.0,
+    ):
+        self.host = host
+        self.port = port
+        self.db = db
+        self.password = password
+        self.decode_responses = decode_responses
+        self.socket_timeout = socket_timeout
+        self.socket_connect_timeout = socket_connect_timeout
+
+
+class RedisConnection:
+    """Redis connection manager with connection pooling."""
+    
+    _instance: Optional[Redis] = None
+    _pool: Optional[redis.ConnectionPool] = None
+    
+    @classmethod
+    def get_connection(cls, config: Optional[RedisConfig] = None) -> Redis:
+        """Get or create Redis connection with pooling."""
+        if cls._instance is None:
+            config = config or RedisConfig()
+            cls._pool = redis.ConnectionPool(
+                host=config.host,
+                port=config.port,
+                db=config.db,
+                password=config.password,
+                decode_responses=config.decode_responses,
+                socket_timeout=config.socket_timeout,
+                socket_connect_timeout=config.socket_connect_timeout,
+            )
+            cls._instance = Redis(connection_pool=cls._pool)
+            logger.info("redis_connection_established", host=config.host, port=config.port)
+        return cls._instance
+    
+    @classmethod
+    def close(cls):
+        """Close Redis connection."""
+        if cls._instance:
+            cls._instance.close()
+            cls._instance = None
+        if cls._pool:
+            cls._pool.disconnect()
+            cls._pool = None
+            logger.info("redis_connection_closed")
+
+
+class RedisStreamsQueue:
+    """Priority queue implementation using Redis Streams."""
+    
+    STREAM_PREFIX = "taskqueue:stream:"
+    JOB_PREFIX = "taskqueue:job:"
+    CONSUMER_GROUP = "taskqueue_workers"
+    
+    def __init__(self, redis_client: Optional[Redis] = None):
+        self._redis = redis_client or RedisConnection.get_connection()
+        self._ensure_consumer_groups()
+    
+    def _ensure_consumer_groups(self):
+        """Create consumer groups for each priority level if they don't exist."""
+        for priority in Priority:
+            stream_key = f"{self.STREAM_PREFIX}{priority.name.lower()}"
+            try:
+                self._redis.xgroup_create(
+                    stream_key, 
+                    self.CONSUMER_GROUP, 
+                    id="0", 
+                    mkstream=True
+                )
+                logger.debug("consumer_group_created", stream=stream_key)
+            except redis.ResponseError as e:
+                if "BUSYGROUP" not in str(e):
+                    raise
+    
+    def enqueue(self, job: Job) -> str:
+        """Add job to Redis Stream based on priority."""
+        priority = Priority(job.priority) if isinstance(job.priority, int) else job.priority
+        stream_key = f"{self.STREAM_PREFIX}{priority.name.lower()}"
+        job_key = f"{self.JOB_PREFIX}{job.id}"
+        
+        job_data = job.model_dump_json()
+        self._redis.set(job_key, job_data)
+        
+        message_id = self._redis.xadd(
+            stream_key,
+            {"job_id": job.id, "created_at": time.time()},
+        )
+        
+        logger.info(
+            "job_enqueued",
+            job_id=job.id,
+            priority=priority.name,
+            stream=stream_key,
+            message_id=message_id,
+        )
+        return message_id
+    
+    def dequeue(
+        self, 
+        consumer_name: str,
+        timeout_ms: int = 5000,
+        count: int = 1,
+    ) -> List[Tuple[str, Job]]:
+        """Dequeue jobs from Redis Streams in priority order."""
+        jobs = []
+        
+        for priority in Priority:
+            if len(jobs) >= count:
+                break
+                
+            stream_key = f"{self.STREAM_PREFIX}{priority.name.lower()}"
+            
+            try:
+                messages = self._redis.xreadgroup(
+                    self.CONSUMER_GROUP,
+                    consumer_name,
+                    {stream_key: ">"},
+                    count=count - len(jobs),
+                    block=timeout_ms if not jobs else 0,
+                )
+                
+                if messages:
+                    for stream_name, stream_messages in messages:
+                        for message_id, data in stream_messages:
+                            job_id = data.get("job_id")
+                            if job_id:
+                                job = self.get_job(job_id)
+                                if job:
+                                    jobs.append((message_id, job))
+                                    logger.debug(
+                                        "job_dequeued",
+                                        job_id=job_id,
+                                        consumer=consumer_name,
+                                    )
+            except RedisError as e:
+                logger.error("dequeue_error", error=str(e), stream=stream_key)
+        
+        return jobs
+    
+    def acknowledge(self, job: Job, message_id: str):
+        """Acknowledge job completion."""
+        priority = Priority(job.priority) if isinstance(job.priority, int) else job.priority
+        stream_key = f"{self.STREAM_PREFIX}{priority.name.lower()}"
+        
+        self._redis.xack(stream_key, self.CONSUMER_GROUP, message_id)
+        logger.debug("job_acknowledged", job_id=job.id, message_id=message_id)
+    
+    def get_job(self, job_id: str) -> Optional[Job]:
+        """Retrieve job by ID."""
+        job_key = f"{self.JOB_PREFIX}{job_id}"
+        job_data = self._redis.get(job_key)
+        
+        if job_data:
+            return Job.model_validate_json(job_data)
+        return None
+    
+    def update_job(self, job: Job):
+        """Update job in Redis."""
+        job_key = f"{self.JOB_PREFIX}{job.id}"
+        self._redis.set(job_key, job.model_dump_json())
+    
+    def delete_job(self, job_id: str):
+        """Delete job from Redis."""
+        job_key = f"{self.JOB_PREFIX}{job_id}"
+        self._redis.delete(job_key)
+    
+    def get_queue_depth(self, priority: Optional[Priority] = None) -> Dict[Priority, int]:
+        """Get queue depth per priority level."""
+        depths = {}
+        
+        priorities = [priority] if priority else list(Priority)
+        
+        for p in priorities:
+            stream_key = f"{self.STREAM_PREFIX}{p.name.lower()}"
+            try:
+                info = self._redis.xinfo_stream(stream_key)
+                depths[p] = info.get("length", 0)
+            except RedisError:
+                depths[p] = 0
+        
+        return depths
+    
+    def get_pending_count(self, priority: Priority, consumer_name: Optional[str] = None) -> int:
+        """Get count of pending (unacknowledged) messages."""
+        stream_key = f"{self.STREAM_PREFIX}{priority.name.lower()}"
+        
+        try:
+            pending = self._redis.xpending(stream_key, self.CONSUMER_GROUP)
+            return pending.get("pending", 0) if pending else 0
+        except RedisError:
+            return 0
+    
+    def size(self) -> int:
+        """Get total queue size across all priorities."""
+        depths = self.get_queue_depth()
+        return sum(depths.values())
+    
+    def size_by_priority(self) -> Dict[Priority, int]:
+        """Get queue size by priority level."""
+        return self.get_queue_depth()
+    
+    def clear(self):
+        """Clear all jobs from Redis queues."""
+        for priority in Priority:
+            stream_key = f"{self.STREAM_PREFIX}{priority.name.lower()}"
+            try:
+                self._redis.delete(stream_key)
+            except RedisError:
+                pass
+    
+    def remove_job(self, job_id: str):
+        """Remove a specific job from the queue."""
+        job_key = f"{self.JOB_PREFIX}{job_id}"
+        self._redis.delete(job_key)
+
+
+class RedisDistributedLock:
+    """Redis-based distributed locking for coordination."""
+    
+    LOCK_PREFIX = "taskqueue:lock:"
+    
+    def __init__(self, redis_client: Optional[Redis] = None):
+        self._redis = redis_client or RedisConnection.get_connection()
+    
+    def acquire(
+        self, 
+        lock_name: str, 
+        owner_id: str, 
+        ttl_seconds: int = 30,
+        blocking: bool = False,
+        timeout: float = 10.0,
+    ) -> bool:
+        """Acquire a distributed lock."""
+        lock_key = f"{self.LOCK_PREFIX}{lock_name}"
+        
+        if blocking:
+            end_time = time.time() + timeout
+            while time.time() < end_time:
+                if self._try_acquire(lock_key, owner_id, ttl_seconds):
+                    return True
+                time.sleep(0.1)
+            return False
+        else:
+            return self._try_acquire(lock_key, owner_id, ttl_seconds)
+    
+    def _try_acquire(self, lock_key: str, owner_id: str, ttl_seconds: int) -> bool:
+        """Try to acquire lock using SET NX."""
+        result = self._redis.set(
+            lock_key,
+            owner_id,
+            nx=True,
+            ex=ttl_seconds,
+        )
+        
+        if result:
+            logger.debug("lock_acquired", lock=lock_key, owner=owner_id)
+            return True
+        return False
+    
+    def release(self, lock_name: str, owner_id: str) -> bool:
+        """Release a distributed lock (only if owned)."""
+        lock_key = f"{self.LOCK_PREFIX}{lock_name}"
+        
+        lua_script = """
+        if redis.call("get", KEYS[1]) == ARGV[1] then
+            return redis.call("del", KEYS[1])
+        else
+            return 0
+        end
+        """
+        
+        result = self._redis.eval(lua_script, 1, lock_key, owner_id)
+        
+        if result:
+            logger.debug("lock_released", lock=lock_key, owner=owner_id)
+            return True
+        return False
+    
+    def extend(self, lock_name: str, owner_id: str, ttl_seconds: int = 30) -> bool:
+        """Extend lock TTL (only if owned)."""
+        lock_key = f"{self.LOCK_PREFIX}{lock_name}"
+        
+        lua_script = """
+        if redis.call("get", KEYS[1]) == ARGV[1] then
+            return redis.call("expire", KEYS[1], ARGV[2])
+        else
+            return 0
+        end
+        """
+        
+        result = self._redis.eval(lua_script, 1, lock_key, owner_id, ttl_seconds)
+        return bool(result)
+    
+    def is_locked(self, lock_name: str) -> bool:
+        """Check if lock is held."""
+        lock_key = f"{self.LOCK_PREFIX}{lock_name}"
+        return self._redis.exists(lock_key) > 0
+    
+    def get_owner(self, lock_name: str) -> Optional[str]:
+        """Get current lock owner."""
+        lock_key = f"{self.LOCK_PREFIX}{lock_name}"
+        return self._redis.get(lock_key)
+
+
+class RedisLeaderElection:
+    """Redis-based leader election for worker coordination."""
+    
+    LEADER_KEY = "taskqueue:leader"
+    
+    def __init__(
+        self, 
+        worker_id: str, 
+        redis_client: Optional[Redis] = None,
+        ttl_seconds: int = 30,
+    ):
+        self._redis = redis_client or RedisConnection.get_connection()
+        self._worker_id = worker_id
+        self._ttl_seconds = ttl_seconds
+        self._lock = RedisDistributedLock(self._redis)
+        self._is_leader = False
+    
+    def try_become_leader(self) -> bool:
+        """Attempt to become the leader."""
+        self._is_leader = self._lock.acquire(
+            self.LEADER_KEY,
+            self._worker_id,
+            self._ttl_seconds,
+        )
+        
+        if self._is_leader:
+            logger.info("became_leader", worker_id=self._worker_id)
+        
+        return self._is_leader
+    
+    def renew_leadership(self) -> bool:
+        """Renew leadership TTL."""
+        if self._is_leader:
+            renewed = self._lock.extend(
+                self.LEADER_KEY,
+                self._worker_id,
+                self._ttl_seconds,
+            )
+            if not renewed:
+                self._is_leader = False
+                logger.warning("leadership_lost", worker_id=self._worker_id)
+            return renewed
+        return False
+    
+    def resign(self):
+        """Resign from leadership."""
+        if self._is_leader:
+            self._lock.release(self.LEADER_KEY, self._worker_id)
+            self._is_leader = False
+            logger.info("resigned_leadership", worker_id=self._worker_id)
+    
+    @property
+    def is_leader(self) -> bool:
+        return self._is_leader
+    
+    def get_current_leader(self) -> Optional[str]:
+        """Get current leader ID."""
+        return self._lock.get_owner(self.LEADER_KEY)
diff -ruN repository_before/retry.py repository_after/retry.py
--- repository_before/retry.py	1970-01-01 03:00:00
+++ repository_after/retry.py	2026-02-14 21:33:24
@@ -0,0 +1,235 @@
+"""Sophisticated retry mechanism with multiple strategies."""
+from __future__ import annotations
+
+import random
+import time
+from abc import ABC, abstractmethod
+from dataclasses import dataclass
+from typing import Callable, Dict, List, Optional
+
+from .models import Job, JobStatus, RetryConfig, RetryStrategy
+
+
+class RetryStrategyHandler(ABC):
+    """Abstract base class for retry strategies."""
+    
+    @abstractmethod
+    def get_delay_ms(self, attempt: int, config: RetryConfig) -> int:
+        """Calculate delay in milliseconds for the given attempt."""
+        pass
+    
+    @abstractmethod
+    def should_retry(self, attempt: int, config: RetryConfig) -> bool:
+        """Determine if another retry should be attempted."""
+        pass
+
+
+class FixedDelayStrategy(RetryStrategyHandler):
+    """Fixed delay between retry attempts."""
+    
+    def get_delay_ms(self, attempt: int, config: RetryConfig) -> int:
+        delay = config.base_delay_ms
+        if config.jitter:
+            jitter = random.uniform(-0.1, 0.1) * delay
+            delay = int(delay + jitter)
+        return max(0, min(delay, config.max_delay_ms))
+    
+    def should_retry(self, attempt: int, config: RetryConfig) -> bool:
+        return attempt < config.max_attempts
+
+
+class ExponentialBackoffStrategy(RetryStrategyHandler):
+    """Exponential backoff with optional jitter."""
+    
+    def get_delay_ms(self, attempt: int, config: RetryConfig) -> int:
+        delay = config.base_delay_ms * (2 ** (attempt - 1))
+        
+        if config.jitter:
+            jitter = random.uniform(0, 0.5) * delay
+            delay = int(delay + jitter)
+        
+        return min(delay, config.max_delay_ms)
+    
+    def should_retry(self, attempt: int, config: RetryConfig) -> bool:
+        return attempt < config.max_attempts
+
+
+class CustomScheduleStrategy(RetryStrategyHandler):
+    """Custom retry schedule with predefined delays."""
+    
+    def get_delay_ms(self, attempt: int, config: RetryConfig) -> int:
+        if not config.custom_delays_ms:
+            return config.base_delay_ms
+        
+        idx = min(attempt - 1, len(config.custom_delays_ms) - 1)
+        delay = config.custom_delays_ms[idx]
+        
+        if config.jitter:
+            jitter = random.uniform(-0.1, 0.1) * delay
+            delay = int(delay + jitter)
+        
+        return max(0, min(delay, config.max_delay_ms))
+    
+    def should_retry(self, attempt: int, config: RetryConfig) -> bool:
+        if config.custom_delays_ms:
+            return attempt <= len(config.custom_delays_ms)
+        return attempt < config.max_attempts
+
+
+class RetryStrategyFactory:
+    """Factory for creating retry strategy handlers."""
+    
+    _strategies: Dict[RetryStrategy, type] = {
+        RetryStrategy.FIXED: FixedDelayStrategy,
+        RetryStrategy.EXPONENTIAL: ExponentialBackoffStrategy,
+        RetryStrategy.CUSTOM: CustomScheduleStrategy,
+    }
+    
+    @classmethod
+    def create(cls, strategy: RetryStrategy) -> RetryStrategyHandler:
+        handler_cls = cls._strategies.get(strategy)
+        if not handler_cls:
+            raise ValueError(f"Unknown retry strategy: {strategy}")
+        return handler_cls()
+    
+    @classmethod
+    def register(cls, strategy: RetryStrategy, handler_cls: type):
+        cls._strategies[strategy] = handler_cls
+
+
+@dataclass
+class RetryDecision:
+    """Decision about whether and how to retry a job."""
+    should_retry: bool
+    delay_ms: int = 0
+    send_to_dlq: bool = False
+    reason: str = ""
+
+
+class RetryManager:
+    """Manages job retry logic and dead-letter queue routing."""
+    
+    def __init__(
+        self,
+        on_retry: Optional[Callable[[Job, int, int], None]] = None,
+        on_dlq: Optional[Callable[[Job, str], None]] = None,
+        on_failure: Optional[Callable[[Job, str], None]] = None,
+    ):
+        self._on_retry = on_retry
+        self._on_dlq = on_dlq
+        self._on_failure = on_failure
+        self._dlq: List[Job] = []
+    
+    def evaluate(self, job: Job, error: str) -> RetryDecision:
+        """Evaluate if job should be retried based on its configuration."""
+        config = job.retry_config
+        strategy = RetryStrategyFactory.create(RetryStrategy(config.strategy))
+        
+        next_attempt = job.attempt + 1
+        
+        if strategy.should_retry(next_attempt, config):
+            delay_ms = strategy.get_delay_ms(next_attempt, config)
+            return RetryDecision(
+                should_retry=True,
+                delay_ms=delay_ms,
+                send_to_dlq=False,
+                reason=f"Retry attempt {next_attempt}/{config.max_attempts}",
+            )
+        else:
+            return RetryDecision(
+                should_retry=False,
+                delay_ms=0,
+                send_to_dlq=True,
+                reason=f"Max attempts ({config.max_attempts}) exhausted",
+            )
+    
+    def handle_failure(self, job: Job, error: str) -> RetryDecision:
+        """Handle job failure with retry or DLQ routing."""
+        decision = self.evaluate(job, error)
+        
+        job.last_error = error
+        
+        if decision.should_retry:
+            job.status = JobStatus.RETRYING
+            job.attempt += 1
+            
+            if self._on_retry:
+                self._on_retry(job, job.attempt, decision.delay_ms)
+        
+        elif decision.send_to_dlq:
+            job.status = JobStatus.DEAD
+            self._dlq.append(job)
+            
+            if self._on_dlq:
+                self._on_dlq(job, decision.reason)
+            
+            if self._on_failure:
+                self._on_failure(job, error)
+        
+        return decision
+    
+    def get_dlq(self) -> List[Job]:
+        """Get all jobs in the dead-letter queue."""
+        return self._dlq.copy()
+    
+    def get_dlq_size(self) -> int:
+        """Get count of jobs in dead-letter queue."""
+        return len(self._dlq)
+    
+    def remove_from_dlq(self, job_id: str) -> Optional[Job]:
+        """Remove and return a job from the dead-letter queue."""
+        for i, job in enumerate(self._dlq):
+            if job.id == job_id:
+                return self._dlq.pop(i)
+        return None
+    
+    def requeue_from_dlq(self, job_id: str, reset_attempts: bool = True) -> Optional[Job]:
+        """Remove job from DLQ and prepare for requeue."""
+        job = self.remove_from_dlq(job_id)
+        if job:
+            if reset_attempts:
+                job.attempt = 0
+            job.status = JobStatus.PENDING
+            job.last_error = None
+        return job
+    
+    def clear_dlq(self) -> int:
+        """Clear all jobs from the dead-letter queue."""
+        count = len(self._dlq)
+        self._dlq.clear()
+        return count
+
+
+class RetryScheduler:
+    """Schedules retry attempts with proper timing."""
+    
+    def __init__(self, retry_manager: RetryManager):
+        self._retry_manager = retry_manager
+        self._scheduled_retries: Dict[str, float] = {}
+    
+    def schedule_retry(self, job: Job, delay_ms: int) -> float:
+        """Schedule a retry and return the scheduled timestamp."""
+        retry_time = time.time() + (delay_ms / 1000)
+        self._scheduled_retries[job.id] = retry_time
+        return retry_time
+    
+    def get_due_retries(self) -> List[str]:
+        """Get job IDs with retries that are due."""
+        now = time.time()
+        due = [jid for jid, ts in self._scheduled_retries.items() if ts <= now]
+        return due
+    
+    def pop_retry(self, job_id: str) -> Optional[float]:
+        """Remove and return scheduled retry time for job."""
+        return self._scheduled_retries.pop(job_id, None)
+    
+    def cancel_retry(self, job_id: str) -> bool:
+        """Cancel a scheduled retry."""
+        if job_id in self._scheduled_retries:
+            del self._scheduled_retries[job_id]
+            return True
+        return False
+    
+    def get_pending_retry_count(self) -> int:
+        """Get count of pending retries."""
+        return len(self._scheduled_retries)
diff -ruN repository_before/scheduler.py repository_after/scheduler.py
--- repository_before/scheduler.py	1970-01-01 03:00:00
+++ repository_after/scheduler.py	2026-02-14 21:33:24
@@ -0,0 +1,335 @@
+"""Scheduler for delayed and recurring job execution."""
+from __future__ import annotations
+
+import hashlib
+import heapq
+import threading
+import time
+from dataclasses import dataclass, field
+from datetime import datetime, timedelta
+from typing import Callable, Dict, List, Optional, Set
+from zoneinfo import ZoneInfo
+
+from .models import Job, JobStatus
+
+
+class CronExpression:
+    """Parser for cron-like expressions."""
+    
+    def __init__(self, expression: str):
+        self.expression = expression
+        self._parts = self._parse(expression)
+    
+    def _parse(self, expr: str) -> Dict[str, Set[int]]:
+        parts = expr.strip().split()
+        if len(parts) != 5:
+            raise ValueError(f"Invalid cron expression: {expr}")
+        
+        return {
+            "minute": self._parse_field(parts[0], 0, 59),
+            "hour": self._parse_field(parts[1], 0, 23),
+            "day": self._parse_field(parts[2], 1, 31),
+            "month": self._parse_field(parts[3], 1, 12),
+            "weekday": self._parse_field(parts[4], 0, 6),
+        }
+    
+    def _parse_field(self, field: str, min_val: int, max_val: int) -> Set[int]:
+        if field == "*":
+            return set(range(min_val, max_val + 1))
+        
+        values = set()
+        for part in field.split(","):
+            if "/" in part:
+                base, step = part.split("/")
+                step = int(step)
+                if base == "*":
+                    values.update(range(min_val, max_val + 1, step))
+                else:
+                    start = int(base)
+                    values.update(range(start, max_val + 1, step))
+            elif "-" in part:
+                start, end = map(int, part.split("-"))
+                values.update(range(start, end + 1))
+            else:
+                values.add(int(part))
+        
+        return values
+    
+    def matches(self, dt: datetime) -> bool:
+        """Check if datetime matches the cron expression."""
+        return (
+            dt.minute in self._parts["minute"]
+            and dt.hour in self._parts["hour"]
+            and dt.day in self._parts["day"]
+            and dt.month in self._parts["month"]
+            and dt.weekday() in self._parts["weekday"]
+        )
+    
+    def next_run(self, after: datetime) -> datetime:
+        """Calculate next run time after the given datetime."""
+        dt = after.replace(second=0, microsecond=0) + timedelta(minutes=1)
+        
+        for _ in range(366 * 24 * 60):
+            if self.matches(dt):
+                return dt
+            dt += timedelta(minutes=1)
+        
+        raise ValueError("Could not find next run time within a year")
+
+
+@dataclass(order=True)
+class ScheduledItem:
+    """Item in the scheduler's priority queue."""
+    run_at: float
+    job_id: str = field(compare=False)
+    is_recurring: bool = field(compare=False, default=False)
+
+
+class DelayedJobScheduler:
+    """Scheduler for delayed job execution with millisecond precision."""
+    
+    def __init__(self):
+        self._heap: List[ScheduledItem] = []
+        self._jobs: Dict[str, Job] = {}
+        self._lock = threading.RLock()
+        self._unique_keys: Dict[str, str] = {}
+    
+    def schedule(self, job: Job, delay_ms: int = 0) -> bool:
+        """Schedule a job for delayed execution."""
+        with self._lock:
+            if job.unique_key:
+                if job.unique_key in self._unique_keys:
+                    return False
+                self._unique_keys[job.unique_key] = job.id
+            
+            run_at = time.time() + (delay_ms / 1000)
+            
+            if job.scheduled_at:
+                run_at = job.scheduled_at.timestamp()
+            elif job.delay_ms > 0:
+                run_at = time.time() + (job.delay_ms / 1000)
+            
+            item = ScheduledItem(run_at=run_at, job_id=job.id)
+            heapq.heappush(self._heap, item)
+            self._jobs[job.id] = job
+            job.status = JobStatus.SCHEDULED
+            return True
+    
+    def get_due_jobs(self) -> List[Job]:
+        """Get all jobs that are due for execution."""
+        now = time.time()
+        due = []
+        
+        with self._lock:
+            while self._heap and self._heap[0].run_at <= now:
+                item = heapq.heappop(self._heap)
+                job = self._jobs.pop(item.job_id, None)
+                if job:
+                    if job.unique_key and job.unique_key in self._unique_keys:
+                        del self._unique_keys[job.unique_key]
+                    due.append(job)
+        
+        return due
+    
+    def cancel(self, job_id: str) -> Optional[Job]:
+        """Cancel a scheduled job."""
+        with self._lock:
+            job = self._jobs.pop(job_id, None)
+            if job:
+                if job.unique_key and job.unique_key in self._unique_keys:
+                    del self._unique_keys[job.unique_key]
+                self._heap = [item for item in self._heap if item.job_id != job_id]
+                heapq.heapify(self._heap)
+            return job
+    
+    def reschedule(self, job_id: str, new_delay_ms: int) -> bool:
+        """Reschedule a job with a new delay."""
+        with self._lock:
+            job = self._jobs.get(job_id)
+            if not job:
+                return False
+            
+            self._heap = [item for item in self._heap if item.job_id != job_id]
+            heapq.heapify(self._heap)
+            
+            run_at = time.time() + (new_delay_ms / 1000)
+            item = ScheduledItem(run_at=run_at, job_id=job_id)
+            heapq.heappush(self._heap, item)
+            return True
+    
+    def get_scheduled_count(self) -> int:
+        with self._lock:
+            return len(self._jobs)
+    
+    def get_next_due_time(self) -> Optional[float]:
+        with self._lock:
+            if self._heap:
+                return self._heap[0].run_at
+            return None
+    
+    def is_unique_key_used(self, key: str) -> bool:
+        with self._lock:
+            return key in self._unique_keys
+
+
+class RecurringJobScheduler:
+    """Scheduler for cron-like recurring jobs."""
+    
+    def __init__(self):
+        self._jobs: Dict[str, Job] = {}
+        self._cron_expressions: Dict[str, CronExpression] = {}
+        self._next_runs: Dict[str, datetime] = {}
+        self._timezones: Dict[str, ZoneInfo] = {}
+        self._lock = threading.RLock()
+    
+    def register(self, job: Job) -> bool:
+        """Register a recurring job with cron expression."""
+        if not job.cron_expression:
+            return False
+        
+        with self._lock:
+            try:
+                cron = CronExpression(job.cron_expression)
+            except ValueError:
+                return False
+            
+            tz = ZoneInfo(job.timezone) if job.timezone else ZoneInfo("UTC")
+            now = datetime.now(tz)
+            next_run = cron.next_run(now)
+            
+            self._jobs[job.id] = job
+            self._cron_expressions[job.id] = cron
+            self._next_runs[job.id] = next_run
+            self._timezones[job.id] = tz
+            return True
+    
+    def unregister(self, job_id: str) -> Optional[Job]:
+        """Unregister a recurring job."""
+        with self._lock:
+            job = self._jobs.pop(job_id, None)
+            self._cron_expressions.pop(job_id, None)
+            self._next_runs.pop(job_id, None)
+            self._timezones.pop(job_id, None)
+            return job
+    
+    def get_due_jobs(self) -> List[Job]:
+        """Get recurring jobs that are due for execution."""
+        due = []
+        
+        with self._lock:
+            for job_id, next_run in list(self._next_runs.items()):
+                tz = self._timezones.get(job_id, ZoneInfo("UTC"))
+                now = datetime.now(tz)
+                
+                if next_run <= now:
+                    job = self._jobs.get(job_id)
+                    if job:
+                        due.append(job)
+                        cron = self._cron_expressions[job_id]
+                        self._next_runs[job_id] = cron.next_run(now)
+        
+        return due
+    
+    def get_next_run(self, job_id: str) -> Optional[datetime]:
+        with self._lock:
+            return self._next_runs.get(job_id)
+    
+    def get_registered_count(self) -> int:
+        with self._lock:
+            return len(self._jobs)
+
+
+class BulkJobSubmitter:
+    """Handles bulk job submission with transactional semantics."""
+    
+    def __init__(
+        self,
+        delayed_scheduler: DelayedJobScheduler,
+        on_submit: Optional[Callable[[Job], None]] = None,
+    ):
+        self._scheduler = delayed_scheduler
+        self._on_submit = on_submit
+    
+    def submit_batch(
+        self,
+        jobs: List[Job],
+        atomic: bool = True,
+    ) -> tuple[List[str], List[tuple[str, str]]]:
+        """Submit multiple jobs with optional atomic semantics."""
+        if atomic:
+            return self._submit_atomic(jobs)
+        else:
+            return self._submit_best_effort(jobs)
+    
+    def _submit_atomic(self, jobs: List[Job]) -> tuple[List[str], List[tuple[str, str]]]:
+        """Submit all jobs or none (transactional)."""
+        for job in jobs:
+            if job.unique_key and self._scheduler.is_unique_key_used(job.unique_key):
+                return [], [(j.id, "Duplicate unique key in batch" if j.unique_key == job.unique_key else "Batch rolled back") for j in jobs]
+        
+        successful = []
+        for job in jobs:
+            if self._scheduler.schedule(job):
+                successful.append(job.id)
+                if self._on_submit:
+                    self._on_submit(job)
+            else:
+                for sid in successful:
+                    self._scheduler.cancel(sid)
+                return [], [(j.id, "Batch submission failed") for j in jobs]
+        
+        return successful, []
+    
+    def _submit_best_effort(self, jobs: List[Job]) -> tuple[List[str], List[tuple[str, str]]]:
+        """Submit as many jobs as possible."""
+        successful = []
+        failed = []
+        
+        for job in jobs:
+            if self._scheduler.schedule(job):
+                successful.append(job.id)
+                if self._on_submit:
+                    self._on_submit(job)
+            else:
+                failed.append((job.id, "Failed to schedule job"))
+        
+        return successful, failed
+
+
+class UniquenessConstraint:
+    """Manages job uniqueness constraints."""
+    
+    def __init__(self):
+        self._keys: Dict[str, str] = {}
+        self._lock = threading.RLock()
+    
+    def generate_key(self, job: Job) -> str:
+        """Generate a unique key for a job based on its properties."""
+        key_data = f"{job.name}:{job.payload}"
+        return hashlib.sha256(key_data.encode()).hexdigest()[:16]
+    
+    def acquire(self, key: str, job_id: str) -> bool:
+        """Try to acquire a uniqueness constraint."""
+        with self._lock:
+            if key in self._keys:
+                return False
+            self._keys[key] = job_id
+            return True
+    
+    def release(self, key: str) -> bool:
+        """Release a uniqueness constraint."""
+        with self._lock:
+            if key in self._keys:
+                del self._keys[key]
+                return True
+            return False
+    
+    def is_held(self, key: str) -> bool:
+        """Check if a uniqueness constraint is held."""
+        with self._lock:
+            return key in self._keys
+    
+    def get_holder(self, key: str) -> Optional[str]:
+        """Get the job ID holding a uniqueness constraint."""
+        with self._lock:
+            return self._keys.get(key)
diff -ruN repository_before/serialization.py repository_after/serialization.py
--- repository_before/serialization.py	1970-01-01 03:00:00
+++ repository_after/serialization.py	2026-02-14 21:33:24
@@ -0,0 +1,154 @@
+"""Pluggable serialization system supporting JSON, MessagePack, and pickle."""
+from __future__ import annotations
+
+import gzip
+import json
+import pickle
+from abc import ABC, abstractmethod
+from enum import Enum
+from typing import Any, Dict, Optional, Type
+
+try:
+    import msgpack
+    HAS_MSGPACK = True
+except ImportError:
+    HAS_MSGPACK = False
+
+
+class SerializationFormat(str, Enum):
+    """Supported serialization formats."""
+    JSON = "json"
+    MSGPACK = "msgpack"
+    PICKLE = "pickle"
+
+
+class Serializer(ABC):
+    """Abstract base class for serializers."""
+    
+    @abstractmethod
+    def serialize(self, data: Any) -> bytes:
+        """Serialize data to bytes."""
+        pass
+    
+    @abstractmethod
+    def deserialize(self, data: bytes) -> Any:
+        """Deserialize bytes to data."""
+        pass
+
+
+class JSONSerializer(Serializer):
+    """JSON serializer implementation."""
+    
+    def serialize(self, data: Any) -> bytes:
+        return json.dumps(data, default=str).encode("utf-8")
+    
+    def deserialize(self, data: bytes) -> Any:
+        return json.loads(data.decode("utf-8"))
+
+
+class MessagePackSerializer(Serializer):
+    """MessagePack serializer implementation."""
+    
+    def serialize(self, data: Any) -> bytes:
+        if not HAS_MSGPACK:
+            raise ImportError("msgpack is not installed")
+        return msgpack.packb(data, use_bin_type=True, default=str)
+    
+    def deserialize(self, data: bytes) -> Any:
+        if not HAS_MSGPACK:
+            raise ImportError("msgpack is not installed")
+        return msgpack.unpackb(data, raw=False)
+
+
+class PickleSerializer(Serializer):
+    """Pickle serializer implementation."""
+    
+    def serialize(self, data: Any) -> bytes:
+        return pickle.dumps(data)
+    
+    def deserialize(self, data: bytes) -> Any:
+        return pickle.loads(data)
+
+
+class CompressedSerializer(Serializer):
+    """Wrapper that adds gzip compression to any serializer."""
+    
+    def __init__(self, inner: Serializer, compression_level: int = 6):
+        self._inner = inner
+        self._level = compression_level
+    
+    def serialize(self, data: Any) -> bytes:
+        raw = self._inner.serialize(data)
+        return gzip.compress(raw, compresslevel=self._level)
+    
+    def deserialize(self, data: bytes) -> Any:
+        raw = gzip.decompress(data)
+        return self._inner.deserialize(raw)
+
+
+class SerializerFactory:
+    """Factory for creating serializers."""
+    
+    _serializers: Dict[SerializationFormat, Type[Serializer]] = {
+        SerializationFormat.JSON: JSONSerializer,
+        SerializationFormat.MSGPACK: MessagePackSerializer,
+        SerializationFormat.PICKLE: PickleSerializer,
+    }
+    
+    @classmethod
+    def create(
+        cls,
+        format: SerializationFormat = SerializationFormat.JSON,
+        compress: bool = False,
+        compression_level: int = 6,
+    ) -> Serializer:
+        """Create a serializer instance."""
+        serializer_cls = cls._serializers.get(format)
+        if not serializer_cls:
+            raise ValueError(f"Unknown serialization format: {format}")
+        
+        serializer = serializer_cls()
+        
+        if compress:
+            serializer = CompressedSerializer(serializer, compression_level)
+        
+        return serializer
+    
+    @classmethod
+    def register(cls, format: SerializationFormat, serializer_cls: Type[Serializer]):
+        """Register a custom serializer."""
+        cls._serializers[format] = serializer_cls
+
+
+class PayloadEncoder:
+    """Handles job payload encoding/decoding with versioning support."""
+    
+    def __init__(
+        self,
+        serializer: Optional[Serializer] = None,
+        default_format: SerializationFormat = SerializationFormat.JSON,
+    ):
+        self._serializer = serializer or SerializerFactory.create(default_format)
+        self._format = default_format
+    
+    def encode(self, payload: Any, version: int = 1) -> bytes:
+        """Encode a payload with version information."""
+        envelope = {
+            "version": version,
+            "format": self._format.value,
+            "data": payload,
+        }
+        return self._serializer.serialize(envelope)
+    
+    def decode(self, data: bytes) -> tuple[Any, int]:
+        """Decode a payload and return (data, version)."""
+        envelope = self._serializer.deserialize(data)
+        return envelope["data"], envelope["version"]
+    
+    def migrate(self, data: Any, from_version: int, to_version: int, migrators: Dict[int, callable]) -> Any:
+        """Migrate payload from one version to another."""
+        current = data
+        for v in range(from_version, to_version):
+            if v in migrators:
+                current = migrators[v](current)
+        return current
diff -ruN repository_before/worker.py repository_after/worker.py
--- repository_before/worker.py	1970-01-01 03:00:00
+++ repository_after/worker.py	2026-02-14 21:33:24
@@ -0,0 +1,435 @@
+"""Worker node management with heartbeat, work stealing, and leader election."""
+from __future__ import annotations
+
+import asyncio
+import os
+import signal
+import socket
+import threading
+import time
+import uuid
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, List, Optional, Set
+
+from .models import Job, JobResult, JobStatus, WorkerInfo
+
+
+class DistributedLock:
+    """Redis-compatible distributed lock (in-memory for testing)."""
+    
+    _locks: Dict[str, tuple[str, float]] = {}
+    _lock = threading.RLock()
+    
+    @classmethod
+    def acquire(cls, key: str, owner: str, ttl_seconds: float = 30) -> bool:
+        """Try to acquire a lock."""
+        with cls._lock:
+            now = time.time()
+            
+            if key in cls._locks:
+                current_owner, expiry = cls._locks[key]
+                if now < expiry:
+                    return current_owner == owner
+                del cls._locks[key]
+            
+            cls._locks[key] = (owner, now + ttl_seconds)
+            return True
+    
+    @classmethod
+    def release(cls, key: str, owner: str) -> bool:
+        """Release a lock if owned."""
+        with cls._lock:
+            if key in cls._locks:
+                current_owner, _ = cls._locks[key]
+                if current_owner == owner:
+                    del cls._locks[key]
+                    return True
+            return False
+    
+    @classmethod
+    def extend(cls, key: str, owner: str, ttl_seconds: float = 30) -> bool:
+        """Extend lock TTL if owned."""
+        with cls._lock:
+            if key in cls._locks:
+                current_owner, _ = cls._locks[key]
+                if current_owner == owner:
+                    cls._locks[key] = (owner, time.time() + ttl_seconds)
+                    return True
+            return False
+    
+    @classmethod
+    def is_locked(cls, key: str) -> bool:
+        """Check if a lock is held."""
+        with cls._lock:
+            if key in cls._locks:
+                _, expiry = cls._locks[key]
+                return time.time() < expiry
+            return False
+    
+    @classmethod
+    def clear_all(cls):
+        """Clear all locks (for testing)."""
+        with cls._lock:
+            cls._locks.clear()
+
+
+class LeaderElection:
+    """Leader election using distributed locking."""
+    
+    LEADER_LOCK_KEY = "taskqueue:leader"
+    
+    def __init__(self, worker_id: str, ttl_seconds: float = 30):
+        self._worker_id = worker_id
+        self._ttl = ttl_seconds
+        self._is_leader = False
+        self._lock = threading.RLock()
+    
+    def try_become_leader(self) -> bool:
+        """Attempt to become the leader."""
+        with self._lock:
+            if DistributedLock.acquire(self.LEADER_LOCK_KEY, self._worker_id, self._ttl):
+                self._is_leader = True
+                return True
+            return False
+    
+    def maintain_leadership(self) -> bool:
+        """Extend leadership if currently leader."""
+        with self._lock:
+            if self._is_leader:
+                if DistributedLock.extend(self.LEADER_LOCK_KEY, self._worker_id, self._ttl):
+                    return True
+                self._is_leader = False
+            return False
+    
+    def resign(self) -> bool:
+        """Voluntarily resign leadership."""
+        with self._lock:
+            if self._is_leader:
+                DistributedLock.release(self.LEADER_LOCK_KEY, self._worker_id)
+                self._is_leader = False
+                return True
+            return False
+    
+    @property
+    def is_leader(self) -> bool:
+        with self._lock:
+            return self._is_leader
+
+
+@dataclass
+class WorkerNode:
+    """Represents a worker node in the cluster."""
+    info: WorkerInfo
+    _running_jobs: Dict[str, Job] = field(default_factory=dict)
+    _lock: threading.RLock = field(default_factory=threading.RLock)
+    
+    def assign_job(self, job: Job) -> bool:
+        """Assign a job to this worker."""
+        with self._lock:
+            if len(self._running_jobs) >= self.info.max_concurrent_jobs:
+                return False
+            self._running_jobs[job.id] = job
+            job.worker_id = self.info.id
+            job.status = JobStatus.RUNNING
+            self.info.current_jobs.append(job.id)
+            return True
+    
+    def complete_job(self, job_id: str, success: bool) -> Optional[Job]:
+        """Mark a job as complete on this worker."""
+        with self._lock:
+            job = self._running_jobs.pop(job_id, None)
+            if job:
+                if job_id in self.info.current_jobs:
+                    self.info.current_jobs.remove(job_id)
+                if success:
+                    self.info.processed_count += 1
+                else:
+                    self.info.failed_count += 1
+            return job
+    
+    def get_running_jobs(self) -> List[Job]:
+        with self._lock:
+            return list(self._running_jobs.values())
+    
+    def get_available_capacity(self) -> int:
+        with self._lock:
+            return self.info.max_concurrent_jobs - len(self._running_jobs)
+    
+    def update_heartbeat(self):
+        from datetime import datetime
+        self.info.last_heartbeat = datetime.utcnow()
+
+
+class WorkerRegistry:
+    """Registry of active worker nodes."""
+    
+    def __init__(self, heartbeat_timeout_seconds: float = 30):
+        self._workers: Dict[str, WorkerNode] = {}
+        self._heartbeat_timeout = heartbeat_timeout_seconds
+        self._lock = threading.RLock()
+    
+    def register(self, worker: WorkerNode) -> bool:
+        """Register a new worker."""
+        with self._lock:
+            self._workers[worker.info.id] = worker
+            return True
+    
+    def unregister(self, worker_id: str) -> Optional[WorkerNode]:
+        """Unregister a worker."""
+        with self._lock:
+            return self._workers.pop(worker_id, None)
+    
+    def get_worker(self, worker_id: str) -> Optional[WorkerNode]:
+        with self._lock:
+            return self._workers.get(worker_id)
+    
+    def get_all_workers(self) -> List[WorkerNode]:
+        with self._lock:
+            return list(self._workers.values())
+    
+    def get_active_workers(self) -> List[WorkerNode]:
+        """Get workers with recent heartbeats."""
+        from datetime import datetime, timedelta
+        
+        cutoff = datetime.utcnow() - timedelta(seconds=self._heartbeat_timeout)
+        
+        with self._lock:
+            return [
+                w for w in self._workers.values()
+                if w.info.last_heartbeat >= cutoff
+            ]
+    
+    def get_stale_workers(self) -> List[WorkerNode]:
+        """Get workers with expired heartbeats."""
+        from datetime import datetime, timedelta
+        
+        cutoff = datetime.utcnow() - timedelta(seconds=self._heartbeat_timeout)
+        
+        with self._lock:
+            return [
+                w for w in self._workers.values()
+                if w.info.last_heartbeat < cutoff
+            ]
+    
+    def heartbeat(self, worker_id: str) -> bool:
+        """Update worker heartbeat."""
+        with self._lock:
+            worker = self._workers.get(worker_id)
+            if worker:
+                worker.update_heartbeat()
+                return True
+            return False
+    
+    def get_worker_count(self) -> int:
+        with self._lock:
+            return len(self._workers)
+
+
+class WorkStealing:
+    """Work stealing for load balancing across workers."""
+    
+    def __init__(self, registry: WorkerRegistry, threshold: float = 0.3):
+        self._registry = registry
+        self._threshold = threshold
+    
+    def find_overloaded_workers(self) -> List[WorkerNode]:
+        """Find workers with high load."""
+        workers = self._registry.get_active_workers()
+        overloaded = []
+        
+        for worker in workers:
+            capacity = worker.info.max_concurrent_jobs
+            current = len(worker.info.current_jobs)
+            if capacity > 0 and (current / capacity) > (1 - self._threshold):
+                overloaded.append(worker)
+        
+        return overloaded
+    
+    def find_underloaded_workers(self) -> List[WorkerNode]:
+        """Find workers with low load."""
+        workers = self._registry.get_active_workers()
+        underloaded = []
+        
+        for worker in workers:
+            capacity = worker.info.max_concurrent_jobs
+            current = len(worker.info.current_jobs)
+            if capacity > 0 and (current / capacity) < self._threshold:
+                underloaded.append(worker)
+        
+        return underloaded
+    
+    def get_steal_candidates(self, from_worker: WorkerNode) -> List[Job]:
+        """Get jobs that can be stolen from a worker."""
+        jobs = from_worker.get_running_jobs()
+        return [j for j in jobs if j.status == JobStatus.PENDING]
+    
+    def steal_job(self, job: Job, from_worker: WorkerNode, to_worker: WorkerNode) -> bool:
+        """Transfer a job from one worker to another."""
+        stolen = from_worker.complete_job(job.id, success=False)
+        if stolen:
+            if to_worker.assign_job(stolen):
+                return True
+            from_worker.assign_job(stolen)
+        return False
+    
+    def steal_jobs(self, count: int = 1) -> List[str]:
+        """Attempt to steal jobs from overloaded workers to underloaded ones."""
+        stolen_job_ids = []
+        
+        overloaded = self.find_overloaded_workers()
+        underloaded = self.find_underloaded_workers()
+        
+        if not overloaded or not underloaded:
+            return stolen_job_ids
+        
+        for from_worker in overloaded:
+            if len(stolen_job_ids) >= count:
+                break
+            
+            job_ids = list(from_worker.info.current_jobs)
+            for job_id in job_ids:
+                if len(stolen_job_ids) >= count:
+                    break
+                
+                for to_worker in underloaded:
+                    capacity = to_worker.info.max_concurrent_jobs
+                    current = len(to_worker.info.current_jobs)
+                    
+                    if current < capacity:
+                        # Move job reference
+                        if job_id in from_worker.info.current_jobs:
+                            from_worker.info.current_jobs.remove(job_id)
+                            to_worker.info.current_jobs.append(job_id)
+                            stolen_job_ids.append(job_id)
+                            break
+        
+        return stolen_job_ids
+
+
+class GracefulShutdown:
+    """Handles graceful worker shutdown with job reassignment."""
+    
+    def __init__(
+        self,
+        worker: WorkerNode,
+        registry: WorkerRegistry,
+        on_job_reassign: Optional[Callable[[Job], None]] = None,
+    ):
+        self._worker = worker
+        self._registry = registry
+        self._on_job_reassign = on_job_reassign
+        self._shutdown_requested = threading.Event()
+        self._shutdown_complete = threading.Event()
+    
+    def request_shutdown(self):
+        """Signal that shutdown has been requested."""
+        self._shutdown_requested.set()
+    
+    def is_shutdown_requested(self) -> bool:
+        return self._shutdown_requested.is_set()
+    
+    def wait_for_shutdown(self, timeout: Optional[float] = None) -> bool:
+        """Wait for shutdown to complete."""
+        return self._shutdown_complete.wait(timeout)
+    
+    def execute_shutdown(self, timeout_seconds: float = 30) -> List[Job]:
+        """Execute graceful shutdown, returning unfinished jobs."""
+        self._worker.info.status = "draining"
+        
+        start_time = time.time()
+        while time.time() - start_time < timeout_seconds:
+            running = self._worker.get_running_jobs()
+            if not running:
+                break
+            time.sleep(0.1)
+        
+        unfinished = self._worker.get_running_jobs()
+        
+        for job in unfinished:
+            self._worker.complete_job(job.id, success=False)
+            job.status = JobStatus.PENDING
+            job.worker_id = None
+            
+            if self._on_job_reassign:
+                self._on_job_reassign(job)
+        
+        self._registry.unregister(self._worker.info.id)
+        self._worker.info.status = "stopped"
+        self._shutdown_complete.set()
+        
+        return unfinished
+
+
+class WorkerProcess:
+    """Main worker process that processes jobs."""
+    
+    def __init__(
+        self,
+        name: str,
+        handler: Callable[[Job], JobResult],
+        max_concurrent: int = 10,
+        heartbeat_interval: float = 10,
+    ):
+        self._handler = handler
+        self._heartbeat_interval = heartbeat_interval
+        
+        self.info = WorkerInfo(
+            id=str(uuid.uuid4()),
+            name=name,
+            host=socket.gethostname(),
+            port=os.getpid(),
+            max_concurrent_jobs=max_concurrent,
+        )
+        
+        self.node = WorkerNode(info=self.info)
+        self._running = False
+        self._leader_election: Optional[LeaderElection] = None
+    
+    def setup_leader_election(self, ttl_seconds: float = 30):
+        """Enable leader election for this worker."""
+        self._leader_election = LeaderElection(self.info.id, ttl_seconds)
+    
+    async def process_job(self, job: Job) -> JobResult:
+        """Process a single job."""
+        from datetime import datetime
+        
+        start_time = time.time()
+        job.started_at = datetime.utcnow()
+        
+        try:
+            result = self._handler(job)
+            job.completed_at = datetime.utcnow()
+            job.status = JobStatus.COMPLETED
+            return result
+        except Exception as e:
+            job.completed_at = datetime.utcnow()
+            job.status = JobStatus.FAILED
+            job.last_error = str(e)
+            
+            return JobResult(
+                job_id=job.id,
+                success=False,
+                error=str(e),
+                duration_ms=(time.time() - start_time) * 1000,
+            )
+    
+    def start(self):
+        """Start the worker."""
+        self._running = True
+        self.info.status = "active"
+    
+    def stop(self):
+        """Stop the worker."""
+        self._running = False
+        if self._leader_election:
+            self._leader_election.resign()
+    
+    @property
+    def is_running(self) -> bool:
+        return self._running
+    
+    @property
+    def is_leader(self) -> bool:
+        if self._leader_election:
+            return self._leader_election.is_leader
+        return False
